{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmcc007/ContinuousIntegrationExample/blob/master/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ijGzTHJJUCPY"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDsTUvKjwHBW"
      },
      "source": [
        "# Building a DIY Multimodal Question Answering System with Vertex AI (A Beginner's Guide - Multimodal RAG)\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fqa-ops%2Fbuilding_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>    \n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_diy_multimodal_qa_system_with_mrag.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_diy_multimodal_qa_system_with_mrag.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_diy_multimodal_qa_system_with_mrag.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_diy_multimodal_qa_system_with_mrag.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_diy_multimodal_qa_system_with_mrag.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR65ni7TRNYG"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Lavi Nigam](https://github.com/lavinigam-gcp) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QduB55tHOa8N"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ This is a new version of the old mRAG notebook with modifications and new data. You can refer to the old notebook here:  ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70SQT5lKO9dp"
      },
      "source": [
        "[**intro_multimodal_rag.ipynb**](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK1Q5ZYdVL4Y"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This guide is your hands-on introduction to creating a question answering system that understands both text and images. We'll build this system from the ground up using Google's Vertex AI, giving you a clear understanding of how it works without relying on complex third-party tools.\n",
        "\n",
        "\n",
        "## Why Build It Yourself?\n",
        "\n",
        "Large Language Models (LLMs) are powerful, but they can seem like a \"black box\". By building our own system, we'll break open that box and explore the core concepts. This will give you the knowledge to customize and optimize every aspect of your question answering system, whether you ultimately choose to code everything yourself or use external libraries.\n",
        "\n",
        "\n",
        "## What We'll Do:\n",
        "\n",
        "* **Focus on Fundamentals**: We'll start with the essential design pattern of \"Retrieval Augmented Generation\" (RAG) – a way to find and use relevant information to answer questions.\n",
        "\n",
        "* **Work with Text and Images**: We'll expand RAG to handle both text and images found in PDF documents. Future guides in this series will explore even more types of data, like videos and audio.\n",
        "\n",
        "* **Use Vertex AI**: We'll only use Google's Vertex AI Embeddings API and Gemini API, ensuring you have complete control and understanding of the building blocks.\n",
        "\n",
        "\n",
        "By the end of this guide, you'll have a solid foundation in building multimodal question answering systems, empowering you to create smarter applications that can understand and respond to a wider range of information.\n",
        "\n",
        "\n",
        "### Gemini\n",
        "\n",
        "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini 1.0 Pro Vision, Gemini 1.0 Pro & Gemini 1.5 Pro models.\n",
        "\n",
        "### Comparing text-based and multimodal RAG\n",
        "\n",
        "Multimodal RAG offers several advantages over text-based RAG:\n",
        "\n",
        "1. **Enhanced knowledge access:** Multimodal RAG can access and process both textual and visual information, providing a richer and more comprehensive knowledge base for the LLM.\n",
        "2. **Improved reasoning capabilities:** By incorporating visual cues, multimodal RAG can make better informed inferences across different types of data modalities.\n",
        "\n",
        "This notebook shows you how to implement DIY RAG with Gemini API in Vertex AI\n",
        " and Vertex AI Embeddings API; [text embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings), and [multimodal embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/multimodal-embeddings), to build a document search engine.\n",
        "\n",
        "Through hands-on examples, you will discover how to construct a multimedia-rich metadata repository of your document sources, enabling search, comparison, and reasoning across diverse information streams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQT500QqVPIb"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "This notebook provides a guide to building a document search engine using multimodal retrieval augmented generation (RAG), step by step:\n",
        "\n",
        "1. Extract and store metadata of documents containing both text and images, and generate embeddings the documents\n",
        "2. Search the metadata with text queries to find similar text or images\n",
        "3. Search the metadata with image queries to find similar images\n",
        "4. Using a text query as input, search for contextual answers using both text and images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnpYxfesh2rI"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXJpXzKrh2rJ"
      },
      "source": [
        "## Getting Started\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5afkyDMSBW5"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kc4WxYmLSBW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba7e5a1-432e-4fd0-f9c7-aad1be229c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /root/.local/lib/python3.10/site-packages (1.75.0)\n",
            "Requirement already satisfied: pymupdf in /root/.local/lib/python3.10/site-packages (1.25.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (13.9.4)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (4.25.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (24.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.14.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.10.3)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (0.16)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich) (4.12.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.6.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.27.1)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --user google-cloud-aiplatform pymupdf rich"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XRvKdaPDTznN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7723a4c4-e6e4-447e-a2c9-85cc61500165"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtsU9Bw9h2rL"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GpYEyLsOh2rL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1vKZZoEh2rL"
      },
      "source": [
        "### Define Google Cloud project information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gJqZ76rJh2rM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f785ed9-4d7f-471a-b260-7ff4edd7974a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your project ID is: lazzloe-web\n"
          ]
        }
      ],
      "source": [
        "# Define project information\n",
        "\n",
        "import sys\n",
        "\n",
        "PROJECT_ID = \"lazzloe-web\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# if not running on Colab, try to get the PROJECT_ID automatically\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    import subprocess\n",
        "\n",
        "    PROJECT_ID = subprocess.check_output(\n",
        "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
        "    ).strip()\n",
        "\n",
        "print(f\"Your project ID is: {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D48gUW5-h2rM"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQwwRiniVFG"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rtMowvm-yQ97"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from rich import print as rich_print\n",
        "from rich.markdown import Markdown as rich_Markdown\n",
        "from vertexai.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Image,\n",
        ")\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "from vertexai.vision_models import MultiModalEmbeddingModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-TX_R_xh2rM"
      },
      "source": [
        "### Load the Gemini 1.5 Pro, Gemini 1.5 Pro Flash, Gemini 1.0 Pro Vision and Gemini 1.0 Pro models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HySgZekYzCpW"
      },
      "source": [
        "Learn more about each models and their differences: [here](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts)\n",
        "\n",
        "Learn about the quotas: [here](https://cloud.google.com/vertex-ai/generative-ai/docs/quotas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SvMwSRJJh2rM"
      },
      "outputs": [],
      "source": [
        "# Instantiate text model with appropriate name and version\n",
        "text_model = GenerativeModel(\"gemini-1.0-pro\")  # works with text, code\n",
        "\n",
        "# Multimodal models: Choose based on your performance/cost needs\n",
        "multimodal_model_15 = GenerativeModel(\n",
        "    \"gemini-1.5-pro\"\n",
        ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - complex reasoning\n",
        "\n",
        "# Multimodal models: Choose based on your performance/cost needs\n",
        "multimodal_model_15_flash = GenerativeModel(\n",
        "    \"gemini-1.5-flash\"\n",
        ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - faster inference\n",
        "\n",
        "multimodal_model_10 = GenerativeModel(\n",
        "    \"gemini-1.0-pro-vision-001\"\n",
        ")  # works with text, code, video(without audio) and images with 16k input context\n",
        "\n",
        "# Load text embedding model from pre-trained source\n",
        "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "\n",
        "# Load multimodal embedding model from pre-trained source\n",
        "multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
        "    \"multimodalembedding\"\n",
        ")  # works with image, image with caption(~32 words), video, video with caption(~32 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7bKCQMFT7JT"
      },
      "source": [
        "#### Get documents and images from GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KwbL89zcY39N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7b2bea0-808c-40e1-cd61-44d0c591e658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: gsutil rsync uses hashes when modification time is not available at\n",
            "both the source and destination. Your crcmod installation isn't using the\n",
            "module's C extension, so checksumming will run very slowly. If this is your\n",
            "first rsync since updating gsutil, this rsync can take significantly longer than\n",
            "usual. For help installing the extension, please see \"gsutil help crcmod\".\n",
            "\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# download documents and images used in this notebook - will take ~30 sec\n",
        "!gsutil -m -q rsync -r gs://github-repo/rag/intro_multimodal_rag/intro_multimodal_rag_v2 .\n",
        "print(\"Download completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps1G-cCfpibN"
      },
      "source": [
        "## Building metadata of documents containing text and images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7uv_PVR1T6B"
      },
      "source": [
        "### The data\n",
        "\n",
        "The source data that you will use in this notebook are:\n",
        "\n",
        "\n",
        "* [Google Cloud TPU Scaling blog](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/Google%20Cloud%20TPU%20blog.pdf)\n",
        "* [Gemini 1.5 Technical Report](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/gemini_v1_5_report_technical.pdf)\n",
        "* [Google Gemma Technical Paper](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/gemma_technical_paper.pdf)\n",
        "* [Med-Gemini Technical Paper](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/med_gemini.pdf)\n",
        "\n",
        "\n",
        "\n",
        "You can also use your data, by first deleting the current files and then placing your files in the `data/` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvt0sus5KSNX"
      },
      "source": [
        "### Import helper functions to build metadata\n",
        "\n",
        "Before building the Multimodal Question Answering System with Vertex AI, it's important to have metadata of all the text and images in the document. For references and citations purposes, the metadata should contain essential elements, including page number, file name, image counter, and so on. Hence, as a next step, you will generate embeddings from the metadata, which is required to perform similarity search when querying the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tStqXX32RNYK"
      },
      "outputs": [],
      "source": [
        "from multimodal_qa_with_rag_utils import get_document_metadata, set_global_variable\n",
        "\n",
        "set_global_variable(\"text_embedding_model\", text_embedding_model)\n",
        "set_global_variable(\"multimodal_embedding_model\", multimodal_embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtaDuBXhFmkL"
      },
      "source": [
        " You can also view the code (`multimodal_qa_with_rag_utils`) [directly](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/utils/multimodal_qa_with_rag_utils.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BOAkYN0KlSL"
      },
      "source": [
        "### Extract and store metadata of text and images from a document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9hBPPWs5CMd"
      },
      "source": [
        "You just imported a function called `get_document_metadata()`. This function extracts text and image metadata from a document, and returns two dataframes, namely *text_metadata* and *image_metadata*, as outputs. If you want to find out more about how `get_document_metadata()` function is implemented using Gemini and the embedding models, you can take look at the [source code](https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/utils/intro_multimodal_rag_utils.py) directly.\n",
        "\n",
        "The reason for extraction and storing both text metadata and image metadata is that just by using either of the two alone is not sufficient to come out with a relevent answer. For example, the relevant answers could be in visual form within a document, but text-based RAG won't be able to take into consideration of the visual images. You will also be exploring this example later in this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnKru0sBh2rN"
      },
      "source": [
        "At the next step, you will use the function to extract and store metadata of text and images from a document. Please note that the following cell may take a few minutes to complete:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8h0XSG_7e5M"
      },
      "source": [
        "**NOTE: Given that we are loading 4 files with roughly 200 pages and approximately 84 images, the cell below will take approximately 7 minutes to run. We recommend loading pre-computed metadata instead.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "X8hE0tWD-lf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765989da-76e0-4bfd-a687-bd840473a72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing pre-existing images folder, since you are running the logic from scratch\n",
            "\n",
            "\n",
            " Processing the file: --------------------------------- data/med_gemini.pdf \n",
            "\n",
            "\n",
            "Processing page: 1\n",
            "Extracting image from page: 1, saved as: images/med_gemini.pdf_image_0_0_46.jpeg\n",
            "Extracting image from page: 1, saved as: images/med_gemini.pdf_image_0_1_48.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 2\n",
            "Extracting image from page: 2, saved as: images/med_gemini.pdf_image_1_0_73.jpeg\n",
            "Extracting image from page: 2, saved as: images/med_gemini.pdf_image_1_1_75.jpeg\n",
            "Extracting image from page: 2, saved as: images/med_gemini.pdf_image_1_2_77.jpeg\n",
            "Extracting image from page: 2, saved as: images/med_gemini.pdf_image_1_3_79.jpeg\n",
            "Extracting image from page: 2, saved as: images/med_gemini.pdf_image_1_4_81.jpeg\n",
            "Extracting image from page: 2, saved as: images/med_gemini.pdf_image_1_5_83.jpeg\n",
            "Extracting image from page: 2, saved as: images/med_gemini.pdf_image_1_6_84.jpeg\n",
            "Extracting image from page: 2, saved as: images/med_gemini.pdf_image_1_7_86.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 3\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 4\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 5\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 6\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 7\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 8\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 9\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 10\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 11\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 12\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 13\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 14\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 15\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 16\n",
            "Extracting image from page: 16, saved as: images/med_gemini.pdf_image_15_0_464.jpeg\n",
            "Extracting image from page: 16, saved as: images/med_gemini.pdf_image_15_1_465.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 17\n",
            "Extracting image from page: 17, saved as: images/med_gemini.pdf_image_16_0_480.jpeg\n",
            "Extracting image from page: 17, saved as: images/med_gemini.pdf_image_16_1_481.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 18\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 19\n",
            "Extracting image from page: 19, saved as: images/med_gemini.pdf_image_18_0_573.jpeg\n",
            "Extracting image from page: 19, saved as: images/med_gemini.pdf_image_18_1_575.jpeg\n",
            "Extracting image from page: 19, saved as: images/med_gemini.pdf_image_18_2_576.jpeg\n",
            "Extracting image from page: 19, saved as: images/med_gemini.pdf_image_18_3_578.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 20\n",
            "Extracting image from page: 20, saved as: images/med_gemini.pdf_image_19_0_613.jpeg\n",
            "Extracting image from page: 20, saved as: images/med_gemini.pdf_image_19_1_615.jpeg\n",
            "Extracting image from page: 20, saved as: images/med_gemini.pdf_image_19_2_617.jpeg\n",
            "Extracting image from page: 20, saved as: images/med_gemini.pdf_image_19_3_618.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 21\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 22\n",
            "Extracting image from page: 22, saved as: images/med_gemini.pdf_image_21_0_681.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 23\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 24\n",
            "Extracting image from page: 24, saved as: images/med_gemini.pdf_image_23_0_726.jpeg\n",
            "Extracting image from page: 24, saved as: images/med_gemini.pdf_image_23_1_727.jpeg\n",
            "Extracting image from page: 24, saved as: images/med_gemini.pdf_image_23_2_728.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 25\n",
            "Extracting image from page: 25, saved as: images/med_gemini.pdf_image_24_0_752.jpeg\n",
            "Extracting image from page: 25, saved as: images/med_gemini.pdf_image_24_1_754.jpeg\n",
            "Extracting image from page: 25, saved as: images/med_gemini.pdf_image_24_2_780.jpeg\n",
            "Extracting image from page: 25, saved as: images/med_gemini.pdf_image_24_3_782.jpeg\n",
            "Extracting image from page: 25, saved as: images/med_gemini.pdf_image_24_4_783.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 26\n",
            "Extracting image from page: 26, saved as: images/med_gemini.pdf_image_25_0_812.jpeg\n",
            "Extracting image from page: 26, saved as: images/med_gemini.pdf_image_25_1_813.jpeg\n",
            "Extracting image from page: 26, saved as: images/med_gemini.pdf_image_25_2_815.jpeg\n",
            "Extracting image from page: 26, saved as: images/med_gemini.pdf_image_25_3_817.jpeg\n",
            "Extracting image from page: 26, saved as: images/med_gemini.pdf_image_25_4_819.jpeg\n",
            "Extracting image from page: 26, saved as: images/med_gemini.pdf_image_25_5_820.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 27\n",
            "Extracting image from page: 27, saved as: images/med_gemini.pdf_image_26_0_849.jpeg\n",
            "Extracting image from page: 27, saved as: images/med_gemini.pdf_image_26_1_851.jpeg\n",
            "Extracting image from page: 27, saved as: images/med_gemini.pdf_image_26_2_853.jpeg\n",
            "Extracting image from page: 27, saved as: images/med_gemini.pdf_image_26_3_855.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 28\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_0_890.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_1_891.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_2_892.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_3_893.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_4_894.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_5_895.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_6_896.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_7_897.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_8_899.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_9_901.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_10_903.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_11_905.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_12_907.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_13_909.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_14_910.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_15_911.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_16_912.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_17_913.jpeg\n",
            "Extracting image from page: 28, saved as: images/med_gemini.pdf_image_27_18_914.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 29\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 30\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 31\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 32\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 33\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 34\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 35\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 36\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 37\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 38\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 39\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 40\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 41\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 42\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 43\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 44\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 45\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 46\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 47\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 48\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 49\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 50\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 51\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 52\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 53\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 54\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 55\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 56\n",
            "Extracting image from page: 56, saved as: images/med_gemini.pdf_image_55_0_1493.jpeg\n",
            "Extracting image from page: 56, saved as: images/med_gemini.pdf_image_55_1_1494.jpeg\n",
            "Extracting image from page: 56, saved as: images/med_gemini.pdf_image_55_2_1495.jpeg\n",
            "Extracting image from page: 56, saved as: images/med_gemini.pdf_image_55_3_1496.jpeg\n",
            "Extracting image from page: 56, saved as: images/med_gemini.pdf_image_55_4_1497.jpeg\n",
            "Extracting image from page: 56, saved as: images/med_gemini.pdf_image_55_5_1498.jpeg\n",
            "Extracting image from page: 56, saved as: images/med_gemini.pdf_image_55_6_1499.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 57\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 58\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "\n",
            " \n",
            " Sleeping for  5  sec before processing the next document to avoid quota issues. You can disable it: \"add_sleep_after_document = False\"  \n",
            "\n",
            "\n",
            " Processing the file: --------------------------------- data/gemma_technical_paper.pdf \n",
            "\n",
            "\n",
            "Processing page: 1\n",
            "Extracting image from page: 1, saved as: images/gemma_technical_paper.pdf_image_0_0_153.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 2\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 3\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 4\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 5\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 6\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 7\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 8\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 9\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 10\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 11\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 12\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 13\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 14\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 15\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 16\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 17\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "\n",
            " \n",
            " Sleeping for  5  sec before processing the next document to avoid quota issues. You can disable it: \"add_sleep_after_document = False\"  \n",
            "\n",
            "\n",
            " Processing the file: --------------------------------- data/Google Cloud TPU blog.pdf \n",
            "\n",
            "\n",
            "Processing page: 1\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 2\n",
            "Extracting image from page: 2, saved as: images/Google Cloud TPU blog.pdf_image_1_0_10.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 3\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 4\n",
            "Extracting image from page: 4, saved as: images/Google Cloud TPU blog.pdf_image_3_0_18.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 5\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 6\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 7\n",
            "Extracting image from page: 7, saved as: images/Google Cloud TPU blog.pdf_image_6_0_35.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 8\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 9\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 10\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 11\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 12\n",
            "Extracting image from page: 12, saved as: images/Google Cloud TPU blog.pdf_image_11_0_62.jpeg\n",
            "Extracting image from page: 12, saved as: images/Google Cloud TPU blog.pdf_image_11_1_63.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 13\n",
            "Extracting image from page: 13, saved as: images/Google Cloud TPU blog.pdf_image_12_0_66.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 14\n",
            "Extracting image from page: 14, saved as: images/Google Cloud TPU blog.pdf_image_13_0_69.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 15\n",
            "Extracting image from page: 15, saved as: images/Google Cloud TPU blog.pdf_image_14_0_72.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 16\n",
            "Extracting image from page: 16, saved as: images/Google Cloud TPU blog.pdf_image_15_0_75.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 17\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 18\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "\n",
            " \n",
            " Sleeping for  5  sec before processing the next document to avoid quota issues. You can disable it: \"add_sleep_after_document = False\"  \n",
            "\n",
            "\n",
            " Processing the file: --------------------------------- data/gemini_v1_5_report_technical.pdf \n",
            "\n",
            "\n",
            "Processing page: 1\n",
            "Extracting image from page: 1, saved as: images/gemini_v1_5_report_technical.pdf_image_0_0_1910.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 2\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 3\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 4\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 5\n",
            "Extracting image from page: 5, saved as: images/gemini_v1_5_report_technical.pdf_image_4_0_142.jpeg\n",
            "Extracting image from page: 5, saved as: images/gemini_v1_5_report_technical.pdf_image_4_1_143.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 6\n",
            "Extracting image from page: 6, saved as: images/gemini_v1_5_report_technical.pdf_image_5_0_148.jpeg\n",
            "Extracting image from page: 6, saved as: images/gemini_v1_5_report_technical.pdf_image_5_1_149.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 7\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 8\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 9\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 10\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 11\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 12\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 13\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 14\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 15\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 16\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 17\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 18\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 19\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 20\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 21\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 22\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 23\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 24\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 25\n",
            "Extracting image from page: 25, saved as: images/gemini_v1_5_report_technical.pdf_image_24_0_488.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 26\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 27\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 28\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 29\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 30\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 31\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 32\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 33\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 34\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 35\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 36\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 37\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 38\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 39\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 40\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 41\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 42\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 43\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 44\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 45\n",
            "Extracting image from page: 45, saved as: images/gemini_v1_5_report_technical.pdf_image_44_0_606.jpeg\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 46\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 47\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 48\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 49\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 50\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 51\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 52\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 53\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 54\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 55\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 56\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 57\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "Processing page: 58\n",
            "Sleeping for  5  sec before processing the next page to avoid quota issues. You can disable it: \"add_sleep_after_page = False\"  \n",
            "\n",
            " \n",
            " Sleeping for  5  sec before processing the next document to avoid quota issues. You can disable it: \"add_sleep_after_document = False\"  \n",
            "\n",
            "\n",
            " --- Completed processing. ---\n",
            "CPU times: user 23.4 s, sys: 2 s, total: 25.4 s\n",
            "Wall time: 20min 28s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Specify the PDF folder with multiple PDF ~7m\n",
        "\n",
        "print(\n",
        "    \"Removing pre-existing images folder, since you are running the logic from scratch\"\n",
        ")\n",
        "! rm -rf images/\n",
        "\n",
        "pdf_folder_path = \"data/\"  # if running in Vertex AI Workbench.\n",
        "\n",
        "# Specify the image description prompt. Change it\n",
        "# image_description_prompt = \"\"\"Explain what is going on in the image.\n",
        "# If it's a table, extract all elements of the table.\n",
        "# If it's a graph, explain the findings in the graph.\n",
        "# Do not include any numbers that are not mentioned in the image.\n",
        "# \"\"\"\n",
        "\n",
        "image_description_prompt = \"\"\"You are a technical image analysis expert. You will be provided with various types of images extracted from documents like research papers, technical blogs, and more.\n",
        "Your task is to generate concise, accurate descriptions of the images without adding any information you are not confident about.\n",
        "Focus on capturing the key details, trends, or relationships depicted in the image.\n",
        "\n",
        "Important Guidelines:\n",
        "* Prioritize accuracy:  If you are uncertain about any detail, state \"Unknown\" or \"Not visible\" instead of guessing.\n",
        "* Avoid hallucinations: Do not add information that is not directly supported by the image.\n",
        "* Be specific: Use precise language to describe shapes, colors, textures, and any interactions depicted.\n",
        "* Consider context: If the image is a screenshot or contains text, incorporate that information into your description.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Extract text and image metadata from the PDF document\n",
        "text_metadata_df, image_metadata_df = get_document_metadata(\n",
        "    multimodal_model_15,  # we are passing Gemini 1.5 Pro\n",
        "    pdf_folder_path,\n",
        "    image_save_dir=\"images\",\n",
        "    image_description_prompt=image_description_prompt,\n",
        "    embedding_size=1408,\n",
        "    add_sleep_after_page = True, # Uncomment this if you are running into API quota issues\n",
        "    sleep_time_after_page = 5,\n",
        "    add_sleep_after_document=True,  # Uncomment this if you are running into API quota issues\n",
        "    sleep_time_after_document=5,  # Increase the value in seconds, if you are still getting quota issues. It will slow down the processing.\n",
        "    # generation_config = # see next cell\n",
        "    # safety_settings =  # see next cell\n",
        ")\n",
        "\n",
        "print(\"\\n\\n --- Completed processing. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWtOx1wb86Az"
      },
      "source": [
        "If you would like to pass additional parameters to Gemini while building metadata, here are some options:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlNK0o2DRNYK"
      },
      "outputs": [],
      "source": [
        "# # Parameters for Gemini API call.\n",
        "# # reference for parameters: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n",
        "\n",
        "# generation_config=  GenerationConfig(temperature=0.2, max_output_tokens=2048)\n",
        "\n",
        "# # Set the safety settings if Gemini is blocking your content or you are facing \"ValueError(\"Content has no parts\")\" error or \"Exception occurred\" in your data.\n",
        "# # ref for settings and thresholds: https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes\n",
        "\n",
        "# safety_settings = {\n",
        "#                   HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "#                   HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "#                   HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "#                   HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "#                   }\n",
        "\n",
        "# # You can also pass parameters and safety_setting to \"get_gemini_response\" function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW3Ci1IL8wSW"
      },
      "source": [
        "### Load pre-computed metadata of text and images from source document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c10dCy6Cig3H"
      },
      "source": [
        "**If you are facing constant issues with Quota or want to focus on the outputs, you should load pre-computed metadata.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w1AGYOYb0In7"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the pickle file\n",
        "with open(\"mrag_metadata.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Extract the DataFrames\n",
        "text_metadata_df = data[\"text_metadata\"]\n",
        "image_metadata_df = data[\"image_metadata\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miBBoEXwh2rN"
      },
      "source": [
        "#### Inspect the processed text metadata\n",
        "\n",
        "\n",
        "The following cell will produce a metadata table which describes the different parts of text metadata, including:\n",
        "\n",
        "- **text**: the original text from the page\n",
        "- **text_embedding_page**: the embedding of the original text from the page\n",
        "- **chunk_text**: the original text divided into smaller chunks\n",
        "- **chunk_number**: the index of each text chunk\n",
        "- **text_embedding_chunk**: the embedding of each text chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6t3AIGFar8Mo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "6a9961d3-5de1-42a2-8857-1ab45f90258a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   file_name  page_num  \\\n",
              "0  gemma_technical_paper.pdf         1   \n",
              "1  gemma_technical_paper.pdf         1   \n",
              "2  gemma_technical_paper.pdf         1   \n",
              "3  gemma_technical_paper.pdf         1   \n",
              "4  gemma_technical_paper.pdf         1   \n",
              "\n",
              "                                                text  \\\n",
              "0  2024-02-21\\nGemma: Open Models Based on Gemini...   \n",
              "1  2024-02-21\\nGemma: Open Models Based on Gemini...   \n",
              "2  2024-02-21\\nGemma: Open Models Based on Gemini...   \n",
              "3  2024-02-21\\nGemma: Open Models Based on Gemini...   \n",
              "4  2024-02-21\\nGemma: Open Models Based on Gemini...   \n",
              "\n",
              "                                 text_embedding_page  chunk_number  \\\n",
              "0  [0.029665455222129822, 0.043536581099033356, -...             1   \n",
              "1  [0.029665455222129822, 0.043536581099033356, -...             2   \n",
              "2  [0.029665455222129822, 0.043536581099033356, -...             3   \n",
              "3  [0.029665455222129822, 0.043536581099033356, -...             4   \n",
              "4  [0.029665455222129822, 0.043536581099033356, -...             5   \n",
              "\n",
              "                                          chunk_text  \\\n",
              "0  2024-02-21\\nGemma: Open Models Based on Gemini...   \n",
              "1  Gemma, a family of open models\\nbased on Googl...   \n",
              "2  d for dialogue, instruction-following, help-\\n...   \n",
              "3  ce (Cobbe et al.,\\n2021; Hendrycks et al., 202...   \n",
              "4   rigorous evaluation and\\nanalysis of current ...   \n",
              "\n",
              "                                text_embedding_chunk  \n",
              "0  [0.035918254405260086, 0.039294395595788956, -...  \n",
              "1  [0.03788634389638901, 0.05161239951848984, -0....  \n",
              "2  [0.045551132410764694, 0.04680870845913887, -0...  \n",
              "3  [0.017123950645327568, 0.05736316367983818, -0...  \n",
              "4  [0.03978141397237778, 0.029347488656640053, -0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57d664ca-bce5-4960-84e4-7814f240f06e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>page_num</th>\n",
              "      <th>text</th>\n",
              "      <th>text_embedding_page</th>\n",
              "      <th>chunk_number</th>\n",
              "      <th>chunk_text</th>\n",
              "      <th>text_embedding_chunk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gemma_technical_paper.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>2024-02-21\\nGemma: Open Models Based on Gemini...</td>\n",
              "      <td>[0.029665455222129822, 0.043536581099033356, -...</td>\n",
              "      <td>1</td>\n",
              "      <td>2024-02-21\\nGemma: Open Models Based on Gemini...</td>\n",
              "      <td>[0.035918254405260086, 0.039294395595788956, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gemma_technical_paper.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>2024-02-21\\nGemma: Open Models Based on Gemini...</td>\n",
              "      <td>[0.029665455222129822, 0.043536581099033356, -...</td>\n",
              "      <td>2</td>\n",
              "      <td>Gemma, a family of open models\\nbased on Googl...</td>\n",
              "      <td>[0.03788634389638901, 0.05161239951848984, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gemma_technical_paper.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>2024-02-21\\nGemma: Open Models Based on Gemini...</td>\n",
              "      <td>[0.029665455222129822, 0.043536581099033356, -...</td>\n",
              "      <td>3</td>\n",
              "      <td>d for dialogue, instruction-following, help-\\n...</td>\n",
              "      <td>[0.045551132410764694, 0.04680870845913887, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gemma_technical_paper.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>2024-02-21\\nGemma: Open Models Based on Gemini...</td>\n",
              "      <td>[0.029665455222129822, 0.043536581099033356, -...</td>\n",
              "      <td>4</td>\n",
              "      <td>ce (Cobbe et al.,\\n2021; Hendrycks et al., 202...</td>\n",
              "      <td>[0.017123950645327568, 0.05736316367983818, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gemma_technical_paper.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>2024-02-21\\nGemma: Open Models Based on Gemini...</td>\n",
              "      <td>[0.029665455222129822, 0.043536581099033356, -...</td>\n",
              "      <td>5</td>\n",
              "      <td>rigorous evaluation and\\nanalysis of current ...</td>\n",
              "      <td>[0.03978141397237778, 0.029347488656640053, -0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57d664ca-bce5-4960-84e4-7814f240f06e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57d664ca-bce5-4960-84e4-7814f240f06e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57d664ca-bce5-4960-84e4-7814f240f06e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9638603e-8176-4fcd-9816-d77467567fa9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9638603e-8176-4fcd-9816-d77467567fa9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9638603e-8176-4fcd-9816-d77467567fa9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "text_metadata_df",
              "summary": "{\n  \"name\": \"text_metadata_df\",\n  \"rows\": 579,\n  \"fields\": [\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"gemini_v1_5_report_technical.pdf\",\n          \"Google Cloud TPU blog.pdf\",\n          \"gemma_technical_paper.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 1,\n        \"max\": 58,\n        \"num_unique_values\": 58,\n        \"samples\": [\n          1,\n          6,\n          35\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 151,\n        \"samples\": [\n          \"Capabilities of Gemini Models in Medicine\\nAdvanced Text Reasoning\\nMultimodal Understanding\\nLong-context Processing\\nReal-world Utility with Novel Applications \\nMedical video QA\\nSoTA on MedQA (USMLE)\\nSelf-training with \\nweb search integration\\n            \\nGemini\\nMedical specialization\\nFine-tuning & customized \\nencoders\\n            \\nMed-Gemini\\nChain-of-reasoning prompting\\nMed-Gemini Development\\nMedical Benchmarking\\nMultimodal understanding \\nAdvanced reasoning\\nLong-context processing\\nInherited capabilities\\nClinical abstraction\\nN\\nA\\nN\\nA\\nN\\nA\\nGPT-4 results not \\navailable (NA)  due to \\ncontext length limitations\\nMed-Gemini\\n91.1\\nGPT-4\\n90.2\\nMed-PaLM 2\\n86.5\\nMed-PaLM\\n67.2\\nGPT-3.5\\n60.2\\nDRAGON\\n47.5\\nBioLinkBERT\\n45.1\\nNote: re-labelling with expert \\nclinicians suggests 7.4% of the \\nquestions in the dataset have \\nquality issues or ambiguous \\nground truth\\nMultimodal medical dialogue\\nFigure 1 | Overview of our contributions. We introduce Med-Gemini, a family of highly capable, multimodal medical\\nmodels built upon Gemini. We enhance our models clinical reasoning capabilities through self-training and web search\\nintegration, while improving multimodal performance via fine-tuning and customized encoders. Med-Gemini models achieve\\nstate-of-the-art (SoTA) performance on 10 out of 14 medical benchmarks that span text, multimodal, and long-context\\napplications, and surpass the GPT-4 model family on every benchmark where a direct comparison could be made. The bar\\nchart shows the relative percentage gains from our models over prior SoTA across the benchmarks. In particular, on the\\nMedQA (USMLE) benchmark, we attain a new SoTA surpassing our prior best (Med-PaLM 2) by a significant margin of\\n4.6%. Moreover, re-annotation of the dataset with expert clinicians reveals that 7.4% of questions are deemed unfit for\\nevaluation as they either lack key information, have incorrect answers, or support multiple plausible interpretations. We\\naccount for these data quality issues to characterize more precisely the performance of our model. Med-Gemini models\\nexcel in multimodal and long-context capabilities as evidenced by their SoTA performance on several benchmarks including\\nneedle-in-a-haystack retrieval from long, de-identified health records, and medical video question answering benchmarks.\\nMoving beyond benchmarks, we also demonstrate the real-world potential of Med-Gemini through quantitative evaluation\\non medical summarization, referral letter generation, and medical simplification tasks where our models outperform human\\nexperts, in addition to qualitative examples of multimodal medical dialogue.\\n2\\n\",\n          \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\\nDepth (%)\\nVideo Haystack\\nMinutes\\n6\\n12\\n18\\n24\\n30\\n36\\n42\\n48\\n54\\n60\\n0\\n20\\n40\\n60\\n80\\n100\\nVideo\\nUp to 3 hours\\n(2.8M tokens)\\nVideocam\\nText Haystack\\nDepth  (%)\\nTokens\\n32k\\n128k\\n256k\\n512k\\n1M\\n0\\n14\\n29\\n43\\n57\\n71\\n86\\n100\\n2M\\n5M\\n10M\\nTokens\\nBook_4\\nText\\nUp to 7M words\\n(10M tokens)\\nSuccessful retrieval\\nUnsuccessful retrieval\\nMinutes\\n36\\n72\\n108 144 180\\nAudio Haystack\\nMinutes \\n12\\n60\\n108\\n156\\n204\\n252\\n300\\n348\\n396\\n444\\n492\\n540\\n636\\n588\\n0\\n20\\n40\\n60\\n80\\n100\\nDepth (%)\\npodcasts_app\\nAudio\\nUp to 22 hours\\n(2M tokens)\\nMinutes \\n840\\n1080\\n1320\\nFigure 1 | Gemini 1.5 Pro achieves near-perfect needle recall (>99.7%) up to 1M tokens of haystack\\nin all modalities, i.e., text, video and audio. It even maintains this recall performance when extending\\nto 10M tokens in the text modality (approximately 7M words); 2M tokens in the audio modality (up\\nto 22 hours); 2.8M tokens in the video modality (up to 3 hours). The x-axis represents the context\\nwindow, and the y-axis the depth percentage of the needle placed for a given context length. The\\nresults are color-coded to indicate: green for successful retrievals and red for unsuccessful ones.\\nTo measure the effectiveness of our models long-context capabilities, we conduct experiments on\\nboth synthetic and real-world tasks. In synthetic needle-in-a-haystack tasks inspired by Kamradt\\n(2023) that probe how reliably the model can recall information amidst distractor context, we\\nfind that Gemini 1.5 Pro achieves near-perfect (>99%) needle recall up to multiple millions of\\ntokens of haystack in all modalities, i.e., text, video and audio, and even maintaining this recall\\nperformance when extending to 10M tokens in the text modality. In more realistic multimodal\\nlong-context benchmarks which require retrieval and reasoning over multiple parts of the context\\n(such as answering questions from long documents or long videos), we also see Gemini 1.5 Pro\\noutperforming all competing models across all modalities even when these models are augmented\\nwith external retrieval methods. Finally, we qualitatively showcase the in-context learning abilities of\\nGemini 1.5 Pro enabled by very long context: for example, learning to translate a new language from\\na single set of linguistic documentation. With only instructional materials (500 pages of linguistic\\ndocumentation, a dictionary, and 400 parallel sentences) all provided in context, Gemini 1.5 Pro\\nis capable of learning to translate from English to Kalamang, a language spoken by fewer than 200\\nspeakers in western New Guinea in the east of Indonesian Papua2, and therefore almost no online\\npresence. Moreover, we find that the quality of its translations is comparable to that of a person who\\nhas learned from the same materials.\\n2Kalamang Language: https://endangeredlanguages.com/lang/1891\\n2\\n\",\n          \"Capabilities of Gemini Models in Medicine\\n In-context demonstrations: For each type of reasoning response path, we hand-curate five\\nexpert demonstrations as seed with accurate clinical reasoning, explaining why the ground-truth\\nanswer is the best suited versus other potentially valid answers. For question examples with\\nsearch results, the demonstrations explicitly refer to, and quote, the helpful information in the\\nsearch results to best answer the question.\\n Generating CoTs: We prompt Med-Gemini-L 1.0 to generate CoTs using the in-context seed\\ndemonstrations over the train set. Before fine-tuning the model on the generated CoTs, we filter\\nout the ones that lead to erroneous predictions.\\n Fine-tuning loop: After fine-tuning Med-Gemini-L 1.0 on the generated CoTs, the models\\nability to follow the reasoning style and search integration of expert demonstrations improves.\\nWe then use the improved model to re-generate the CoTs, and iteratively repeat this self-training\\nprocess until the models performance saturates.\\nBelow we provide a MedQA-RS example of an input prompt, along with the retrieved search results\\nand an example of a generated CoT, which is then used to further fine-tune Med-Gemini-L 1.0. For\\nbrevity, we only display one representative search result in the example below.\\nInput\\nInstruction\\nYou are a medical expert answering a multiple choice question about medical knowledge. To help you answer the\\nquestion, you are given access to search results.\\nQuestion\\nA 20-year-old woman presents with menorrhagia for the past several years. She says that her menses have always\\nbeen heavy, and she has experienced easy bruising for as long as she can remember. Family history is significant\\nfor her mother, who had similar problems with bruising easily. The patients vital signs include: heart rate 98/min,\\nrespiratory rate 14/min, temperature 36.1C (96.9F), and blood pressure 110/87 mm Hg. Physical examination is\\nunremarkable. Laboratory tests show the following: platelet count 200,000/mm3, PT 12 seconds, and PTT 43\\nseconds. Which of the following is the most likely cause of this patients symptoms? (A) Hemophilia A (B) Lupus\\nanticoagulant (C) Protein C deficiency (D) Von Willebrand disease\\nSearch Results\\nPartial thromboplastin time (PTT) and activated partial thromboplastin time (aPTT) are used to test for the same\\nfunctions; however, in aPTT, an activator is added that speeds up the clotting time and results in a narrower\\nreference range. The aPTT is considered a more sensitive version of the PTT and is used to monitor the patients\\nresponse to heparin therapy. The reference range of the aPTT is 30-40 seconds. The reference range of the PTT is\\n60-70 seconds. In patients receiving anticoagulant therapy, the reference range is 1.5-2.5 times the control value in\\nseconds. Critical values that should prompt a clinical alert are as follows: aPTT: More than 70 seconds (signifies\\nspontaneous bleeding). PTT: More than 100 seconds (signifies spontaneous bleeding). eMedicine Logo. Next:\\nInterpretation. A prolonged aPTT result may indicate the following : Congenital deficiencies of intrinsic system\\nclotting factors such as factors VIII, IX, XI, and XII, including hemophilia A and hemophilia B (Christmas disease),\\ntwo inherited bleeding disorders resulting from a deficiency in factors VIII and IX, respectively. Congenital deficiency\\nof Fitzgerald factor (prekallikrein). Von Willebrand disease, which is the most common inherited bleeding disorder,\\naffecting platelet function owing to decreased von Willebrand factor activity. Hypofibrinogenemia  Liver cirrhosis\\n(the liver makes most of the clotting factors, including those that are vitamin K-dependent ones); diseases of the liver\\nmay result in an inadequate quantity of clotting factors, prolonging the aPTT. Vitamin K deficiency: The synthesis\\nof some clotting factors requires vitamin K, so vitamin K deficiency results in an inadequate quantity of intrinsic\\nsystem and common pathways clotting factors, as a result the aPTT is prolonged. Disseminated intravascular\\ncoagulation (DIC): The clotting factors involved in the intrinsic pathway are consumed, prolonging the aPTT.\\nSource: https://emedicine.medscape.com/article/2085837-overview\\n8\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_embedding_page\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 8,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2,\n          6,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 579,\n        \"samples\": [\n          \"he\\nprefix and suffix are not found, we search for a LaTeX string in the format \\\\boxed{answer}. If the\\nextracted answer doesnt exactly match the target answer, we employ a symbolic parser implemented\\n48\\n\",\n          \"Capabilities of Gemini Models in Medicine\\nsplit as described in the MIMIC-CXR for all tasks. We consider four fine-tuning tasks using\\nMIMIC-CXR: (1) normal vs. abnormal binary classification, (2) CXR abnormality condition\\nVQA, (3) synthetic CXR VQA, and (4) text report generation. For the normal vs. abnormal\\nbinary classification task, we classify each image into either normal or abnoraml category\\nbased on the CheXpert no finding label using all frontal view images [anterior-posterior\\n(AP) and posterioranterior (PA) views] with the task prompt listed in Figure D1. For CXR\\nabnormality condition VQA, we exclude all images with normal findings, and group positive\\nand uncertain labels as positive class for 13 abnormal conditions: atelectasis, cardiomegaly,\\nconsolidation, edema, enlarged cardiomediastinum, fracture, lung lesion, lung opacity, pleural\\neffusion, pleural other, pneumonia, pneumothorax, and support devices. Then we frame\\nthe abnormality detection problem into a close-ended mult\",\n          \"it), of total peak performance. To give a sense of scale, this cluster of\\nCloud TPU v5e chips has more AI accelerators than the TOP1 Supercomputer Frontier\\nat Oak Ridge National Laboratory, which featured 37,888 AMD M1250X GPUs.\\nSetting up the LLM distributed training job on Cloud TPU v5e\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_embedding_chunk\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "text_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjIQYI3mh2rO"
      },
      "source": [
        "#### Inspect the processed image metadata\n",
        "\n",
        "The following cell will produce a metadata table which describes the different parts of image metadata, including:\n",
        "* **img_desc**: Gemini-generated textual description of the image.\n",
        "* **mm_embedding_from_text_desc_and_img**: Combined embedding of image and its description, capturing both visual and textual information.\n",
        "* **mm_embedding_from_img_only**: Image embedding without description, for comparison with description-based analysis.\n",
        "* **text_embedding_from_image_description**: Separate text embedding of the generated description, enabling textual analysis and comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tkHtAYIK-y-q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "8cf3d640-701f-47cb-af2d-6b56bff63a14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          file_name  page_num  img_num  \\\n",
              "0         gemma_technical_paper.pdf         1        1   \n",
              "1  gemini_v1_5_report_technical.pdf         1        1   \n",
              "2  gemini_v1_5_report_technical.pdf         5        1   \n",
              "3  gemini_v1_5_report_technical.pdf         5        2   \n",
              "4  gemini_v1_5_report_technical.pdf         6        1   \n",
              "\n",
              "                                            img_path  \\\n",
              "0  images/gemma_technical_paper.pdf_image_0_0_153...   \n",
              "1  images/gemini_v1_5_report_technical.pdf_image_...   \n",
              "2  images/gemini_v1_5_report_technical.pdf_image_...   \n",
              "3  images/gemini_v1_5_report_technical.pdf_image_...   \n",
              "4  images/gemini_v1_5_report_technical.pdf_image_...   \n",
              "\n",
              "                                            img_desc  \\\n",
              "0  The image shows the Google logo in its standar...   \n",
              "1  The image shows the Google logo in its standar...   \n",
              "2  The image depicts a user prompt asking for the...   \n",
              "3  The image depicts a system designed to transla...   \n",
              "4  The image depicts a user interface with three ...   \n",
              "\n",
              "                          mm_embedding_from_img_only  \\\n",
              "0  [-0.0264111869, 0.0349279381, -0.0139624262, -...   \n",
              "1  [-0.0264111571, 0.0349279791, -0.0139624029, -...   \n",
              "2  [-0.0133379065, 0.0248864833, -0.0143319033, 0...   \n",
              "3  [-0.0333386473, 0.0493280329, 0.00310201081, 0...   \n",
              "4  [-0.0436088592, 0.00902390946, -0.0243023187, ...   \n",
              "\n",
              "               text_embedding_from_image_description  \n",
              "0  [0.025234034284949303, 0.040900733321905136, -...  \n",
              "1  [0.025234034284949303, 0.040900733321905136, -...  \n",
              "2  [0.04414892569184303, 0.008230694569647312, -0...  \n",
              "3  [-0.0197904035449028, 0.011377139948308468, -0...  \n",
              "4  [0.018790962174534798, 0.014835771173238754, -...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43abb839-2154-403a-bead-f4c628306309\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>page_num</th>\n",
              "      <th>img_num</th>\n",
              "      <th>img_path</th>\n",
              "      <th>img_desc</th>\n",
              "      <th>mm_embedding_from_img_only</th>\n",
              "      <th>text_embedding_from_image_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gemma_technical_paper.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>images/gemma_technical_paper.pdf_image_0_0_153...</td>\n",
              "      <td>The image shows the Google logo in its standar...</td>\n",
              "      <td>[-0.0264111869, 0.0349279381, -0.0139624262, -...</td>\n",
              "      <td>[0.025234034284949303, 0.040900733321905136, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gemini_v1_5_report_technical.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>images/gemini_v1_5_report_technical.pdf_image_...</td>\n",
              "      <td>The image shows the Google logo in its standar...</td>\n",
              "      <td>[-0.0264111571, 0.0349279791, -0.0139624029, -...</td>\n",
              "      <td>[0.025234034284949303, 0.040900733321905136, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gemini_v1_5_report_technical.pdf</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>images/gemini_v1_5_report_technical.pdf_image_...</td>\n",
              "      <td>The image depicts a user prompt asking for the...</td>\n",
              "      <td>[-0.0133379065, 0.0248864833, -0.0143319033, 0...</td>\n",
              "      <td>[0.04414892569184303, 0.008230694569647312, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gemini_v1_5_report_technical.pdf</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>images/gemini_v1_5_report_technical.pdf_image_...</td>\n",
              "      <td>The image depicts a system designed to transla...</td>\n",
              "      <td>[-0.0333386473, 0.0493280329, 0.00310201081, 0...</td>\n",
              "      <td>[-0.0197904035449028, 0.011377139948308468, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gemini_v1_5_report_technical.pdf</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>images/gemini_v1_5_report_technical.pdf_image_...</td>\n",
              "      <td>The image depicts a user interface with three ...</td>\n",
              "      <td>[-0.0436088592, 0.00902390946, -0.0243023187, ...</td>\n",
              "      <td>[0.018790962174534798, 0.014835771173238754, -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43abb839-2154-403a-bead-f4c628306309')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-43abb839-2154-403a-bead-f4c628306309 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-43abb839-2154-403a-bead-f4c628306309');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-24aee1b4-2353-4318-b0eb-2fa4474e1bf7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24aee1b4-2353-4318-b0eb-2fa4474e1bf7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-24aee1b4-2353-4318-b0eb-2fa4474e1bf7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "image_metadata_df",
              "summary": "{\n  \"name\": \"image_metadata_df\",\n  \"rows\": 80,\n  \"fields\": [\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"gemini_v1_5_report_technical.pdf\",\n          \"Google Cloud TPU blog.pdf\",\n          \"gemma_technical_paper.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 1,\n        \"max\": 56,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          1,\n          12,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"img_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 1,\n        \"max\": 19,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          1,\n          6,\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"img_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 80,\n        \"samples\": [\n          \"images/med_gemini.pdf_image_23_1_727.jpeg\",\n          \"images/gemma_technical_paper.pdf_image_0_0_153.jpeg\",\n          \"images/med_gemini.pdf_image_18_0_573.jpeg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"img_desc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 79,\n        \"samples\": [\n          \"The image shows a surgical field, likely during laparoscopic surgery. A dark red organ, likely the liver, is partially visible. A tubular structure, likely a bile duct, is being held with surgical instruments. A surgical instrument with a white tip is visible near the tubular structure. The surrounding tissue is red and appears to be dissected. \\n\",\n          \"The image shows the Google logo in its standard colors (blue, red, yellow, blue, green, red) immediately followed by the text \\\"DeepMind\\\" in gray. \\n\",\n          \"A close-up image shows a circular wound on a person's leg. The wound has a dark red center with a lighter, scaly outer ring. The surrounding skin is fair-toned with visible pores and fine hairs. \\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mm_embedding_from_img_only\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_embedding_from_image_description\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "image_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBhoOkutUtPr"
      },
      "source": [
        "### Import the helper functions to implement RAG\n",
        "\n",
        "You will be importing the following functions which will be used in the remainder of this notebook to implement RAG:\n",
        "\n",
        "* **get_similar_text_from_query():** Given a text query, finds text from the document which are relevant, using cosine similarity algorithm. It uses text embeddings from the metadata to compute and the results can be filtered by top score, page/chunk number, or embedding size.\n",
        "* **print_text_to_text_citation():** Prints the source (citation) and details of the retrieved text from the `get_similar_text_from_query()` function.\n",
        "* **get_similar_image_from_query():** Given an image path or an image, finds images from the document which are relevant. It uses image embeddings from the metadata.\n",
        "* **print_text_to_image_citation():** Prints the source (citation) and the details of retrieved images from the `get_similar_image_from_query()` function.\n",
        "* **get_gemini_response():** Interacts with a Gemini model to answer questions based on a combination of text and image inputs.\n",
        "* **display_images():**  Displays a series of images provided as paths or PIL Image objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Tngn_vrIKdE1"
      },
      "outputs": [],
      "source": [
        "from multimodal_qa_with_rag_utils import (\n",
        "    display_images,\n",
        "    get_answer_from_qa_system,\n",
        "    get_gemini_response,\n",
        "    get_similar_image_from_query,\n",
        "    get_similar_text_from_query,\n",
        "    print_text_to_image_citation,\n",
        "    print_text_to_text_citation,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9jGEj6DY1Rj"
      },
      "source": [
        "Before implementing a Multimodal Question Answering System with Vertex AI, let's explore what you can achieve with just text or image embeddings. This will set the foundation for implementing a multimodal Retrieval Augmented Generation (RAG) system, which you will do later in this notebook.\n",
        "\n",
        "You can also use these essential elements together to build applications for multimodal use cases, extracting meaningful information from documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHuLlEvSKFWt"
      },
      "source": [
        "## Text Search\n",
        "\n",
        "Let's start the search with a simple question and see if the simple text search using text embeddings can answer it. The expected answer is to show the value of basic and diluted net income per share of Google for different share types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5mrFVhtCut7t"
      },
      "outputs": [],
      "source": [
        "query = \"What are various med-gemini medical benchmarks that shows its performance relative to other models?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWw7-AIar-S8"
      },
      "source": [
        "### Search similar text with text query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEzP6Yyv7N-G",
        "outputId": "e06bf43c-1d9a-4492-84a7-579d28257130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[91mCitation 1: Matched text: \n",
            "\u001b[0m\n",
            "\u001b[94mscore: \u001b[0m 0.78\n",
            "\u001b[94mfile_name: \u001b[0m med_gemini.pdf\n",
            "\u001b[94mpage_number: \u001b[0m 29\n",
            "\u001b[94mchunk_number: \u001b[0m 1\n",
            "\u001b[94mchunk_text: \u001b[0m Capabilities of Gemini Models in Medicine\n",
            "5. Discussion\n",
            "Med-Gemini, built upon the Gemini models, demonstrates significant advancements in clinical\n",
            "reasoning, multimodal understanding, and long-context processing within the medical domain. This\n",
            "is evidenced by its strong performance across a diverse range of 25 tasks spanning 14 medical\n",
            "benchmarks, encompassing medical knowledge, clinical reasoning, genomics, waveforms, medical\n",
            "imaging, health records and videos.\n",
            "MedQA performance\n",
            "Notably, Med-Gemini-L 1.0 achieves a new SoTA on MedQA (USMLE), a\n",
            "popular benchmark for medical question answering with the use of self-training based fine-tuning\n",
            "and search integration. Our thorough relabeling of the MedQA test set (performed by attending\n",
            "clinicians) reveals important insights. While MedQA (USMLE) is a useful benchmark for assessing\n",
            "medical knowledge and reasoning, it is essential to acknowledge its limitations. We discover that\n",
            "approximately 4% of the questions contain missing information, \n"
          ]
        }
      ],
      "source": [
        "# Matching user text query with \"chunk_embedding\" to find relevant chunks.\n",
        "matching_results_text = get_similar_text_from_query(\n",
        "    query,\n",
        "    text_metadata_df,\n",
        "    column_name=\"text_embedding_chunk\",\n",
        "    top_n=3,\n",
        "    chunk_text=True,\n",
        ")\n",
        "\n",
        "# Print the matched text citations\n",
        "print_text_to_text_citation(\n",
        "    matching_results_text, print_top=True, chunk_text=True\n",
        ")  # print_top=False to see all text matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY1J2sHr-N8f"
      },
      "source": [
        "### Get answer with text-RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORCistIdDWoE"
      },
      "outputs": [],
      "source": [
        "# All relevant text chunk found across documents based on user query\n",
        "context = \"\\n\".join(\n",
        "    [value[\"chunk_text\"] for key, value in matching_results_text.items()]\n",
        ")\n",
        "\n",
        "prompt = f\"\"\"Answer the question with the given context. If the specific answer is not in the context, please answer \"I don't know\".\n",
        "Question: {query}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKD-Ew8IM287"
      },
      "outputs": [],
      "source": [
        "safety_settings = {\n",
        "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8jyc5_SAwlF",
        "outputId": "28eacc9d-9ce3-4849-8869-3ed3eb8fb2e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " **** Result: ***** \n",
            "\n",
            "CPU times: user 24 ms, sys: 3.12 ms, total: 27.1 ms\n",
            "Wall time: 1.25 s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The passage mentions that Med-Gemini models are evaluated on 14 medical benchmarks, but it does not enumerate the  \n",
              "specific benchmarks. Therefore, I cannot provide a list of the benchmarks.                                         \n",
              "</pre>\n"
            ],
            "text/plain": [
              "The passage mentions that Med-Gemini models are evaluated on 14 medical benchmarks, but it does not enumerate the  \n",
              "specific benchmarks. Therefore, I cannot provide a list of the benchmarks.                                         \n"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# Generate response with Gemini 1.5 Pro\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "rich_Markdown(\n",
        "    get_gemini_response(\n",
        "        multimodal_model_15,\n",
        "        model_input=prompt,\n",
        "        stream=True,\n",
        "        safety_settings=safety_settings,\n",
        "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rKLLKt1An97",
        "outputId": "ed3e0980-d08e-487e-b064-9611fbbd5a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 26.9 ms, sys: 3.69 ms, total: 30.6 ms\n",
            "Wall time: 964 ms\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The context mentions that Med-Gemini was evaluated on 25 tasks across 14 medical benchmarks, but it doesn't list   \n",
              "the specific benchmarks. Therefore, I don't know the various med-gemini medical benchmarks that show its           \n",
              "performance relative to other models.                                                                              \n",
              "</pre>\n"
            ],
            "text/plain": [
              "The context mentions that Med-Gemini was evaluated on 25 tasks across 14 medical benchmarks, but it doesn't list   \n",
              "the specific benchmarks. Therefore, I don't know the various med-gemini medical benchmarks that show its           \n",
              "performance relative to other models.                                                                              \n"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# Generate response with Gemini 1.5 FLash\n",
        "rich_Markdown(\n",
        "    get_gemini_response(\n",
        "        multimodal_model_15_flash,\n",
        "        model_input=prompt,\n",
        "        stream=True,\n",
        "        safety_settings=safety_settings,\n",
        "        generation_config=GenerationConfig(temperature=0.1),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXm271jdD-Rl"
      },
      "source": [
        "### Search similar images with text query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPxwfyVrr9-G"
      },
      "source": [
        "Since plain text search and RAG didn't provide the detailed answer and the information may be visually represented in a table or another image format, you can use multimodal capability of Gemini 1.0 Pro Vision or Gemini 1.5 Pro model for the similar task.\n",
        "\n",
        "The goal here also is to find an image similar to the text query. You may also print the citations to verify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwXpqMSq4ppr"
      },
      "outputs": [],
      "source": [
        "query = \"What are various med-gemini medical benchmarks that shows its performance relative to other models?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "knj4qQ4xni24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851ef43b-c7f9-40b6-b38a-855109e5b174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " **** Result: ***** \n",
            "\n"
          ]
        }
      ],
      "source": [
        "matching_results_image = get_similar_image_from_query(\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    query=query,\n",
        "    column_name=\"text_embedding_from_image_description\",  # Use image description text embedding\n",
        "    image_emb=False,  # Use text embedding instead of image embedding\n",
        "    top_n=5,\n",
        "    embedding_size=1408,\n",
        ")\n",
        "\n",
        "# Markdown(print_text_to_image_citation(matching_results_image, print_top=True))\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "# Display the top matching image\n",
        "# display_images(\n",
        "#     [\n",
        "#         matching_results_image[0][\"img_path\"],\n",
        "#         matching_results_image[1][\"img_path\"],\n",
        "#         matching_results_image[2][\"img_path\"],\n",
        "#         matching_results_image[3][\"img_path\"],\n",
        "#     ],\n",
        "#     resize_ratio=0.3,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuRD1lZ8RNYP",
        "outputId": "887c9149-6cb4-4427-adb5-83fbcfad0c18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " **** Result: ***** \n",
            "\n",
            "CPU times: user 136 ms, sys: 22.6 ms, total: 158 ms\n",
            "Wall time: 13.7 s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The images provide a comprehensive view of Med-Gemini's performance across various medical benchmarks relative to  \n",
              "other models, primarily focusing on accuracy and expert preference.                                                \n",
              "\n",
              "Here's a breakdown of the insights gleaned from each image and what they tell us about Med-Gemini's capabilities:  \n",
              "\n",
              "<span style=\"font-weight: bold\">Image 1: NEJM CPC Accuracy</span>                                                                                         \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Superior Performance:</span> Med-Gemini, both with and without search augmentation, consistently outperforms prior     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>state-of-the-art (SOTA) models and clinician baselines on the NEJM CPC (New England Journal of Medicine Clinical\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Problem Solving Challenge) task.                                                                                \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Search Augmentation Benefits:</span>  Adding search capabilities boosts the accuracy for both Med-Gemini and clinician \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>performance, highlighting the value of information retrieval in medical reasoning tasks.                        \n",
              "\n",
              "<span style=\"font-weight: bold\">Image 2: Performance Across Tasks</span>                                                                                  \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Generally Strong Performance:</span> While specific tasks are unnamed, Med-Gemini demonstrates superior performance    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>compared to the previous SOTA in most categories.                                                               \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">GPT-4 Comparison:</span> Interestingly, the best GPT-4 method lags behind both Med-Gemini and the prior SOTA in most   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>tasks. This suggests a specialized model like Med-Gemini, fine-tuned on medical data, can outperform more       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>general-purpose language models in the medical domain.                                                          \n",
              "\n",
              "<span style=\"font-weight: bold\">Image 3: Expert Preference</span>                                                                                         \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">High Subjective Preference:</span>  Med-Gemini receives a high percentage of \"preferred\" responses from experts in     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>tasks like Doctor Referral Generation and Medical Simplification. This suggests the model's outputs align well  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>with expert judgment in these areas.                                                                            \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Room for Improvement:</span>  In Medical Summarization, while Med-Gemini is still favored, there's a notable proportion\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>of \"Expert Preferred\" responses. This highlights an area where further refinement could enhance alignment with  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>expert preferences.                                                                                             \n",
              "\n",
              "<span style=\"font-weight: bold\">Image 4: Accuracy in Gene &amp; DNA Tasks</span>                                                                              \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Strengths in Complex Tasks:</span> Med-Gemini showcases strong performance in tasks like \"Protein-coding genes\" and    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>\"Human genome TF regulation,\" demonstrating proficiency in handling intricate biological information.           \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Areas for Attention:</span>  Tasks like \"Gene name conversion\" and \"Multi-species DNA alignment\" show a higher         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>\"Abstain\" rate for Med-Gemini, indicating potential areas where the model may be less confident or the tasks are\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>particularly challenging.                                                                                       \n",
              "\n",
              "<span style=\"font-weight: bold\">Overall, the images paint a picture of Med-Gemini as a high-performing medical AI model.</span> It demonstrates:          \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Superior Accuracy:</span> Outperforming previous SOTA models and clinician baselines in challenging medical tasks.     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Effective Search Augmentation:</span>  Benefitting from incorporating search to improve accuracy further.              \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Expert Alignment:</span>  Generating responses often preferred by medical experts, particularly in referral generation \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>and medical simplification.                                                                                     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Specialization Advantage:</span>  Outperforming even powerful general-purpose language models like GPT-4 in specific   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>medical domains.                                                                                                \n",
              "\n",
              "However, the analysis also reveals areas for potential improvement:                                                \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Enhancing Summarization:</span> Refining the model to better align with expert preferences in summarization tasks.     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Addressing Abstentions:</span>  Investigating and potentially improving performance in tasks where the model frequently\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>abstains, suggesting lower confidence or higher task complexity.                                                \n",
              "\n",
              "In conclusion, Med-Gemini shows significant promise as a valuable tool in various medical applications. Continuous \n",
              "evaluation and refinement in areas needing improvement will further solidify its role in assisting healthcare      \n",
              "professionals and researchers.                                                                                     \n",
              "</pre>\n"
            ],
            "text/plain": [
              "The images provide a comprehensive view of Med-Gemini's performance across various medical benchmarks relative to  \n",
              "other models, primarily focusing on accuracy and expert preference.                                                \n",
              "\n",
              "Here's a breakdown of the insights gleaned from each image and what they tell us about Med-Gemini's capabilities:  \n",
              "\n",
              "\u001b[1mImage 1: NEJM CPC Accuracy\u001b[0m                                                                                         \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSuperior Performance:\u001b[0m Med-Gemini, both with and without search augmentation, consistently outperforms prior     \n",
              "\u001b[1;33m   \u001b[0mstate-of-the-art (SOTA) models and clinician baselines on the NEJM CPC (New England Journal of Medicine Clinical\n",
              "\u001b[1;33m   \u001b[0mProblem Solving Challenge) task.                                                                                \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSearch Augmentation Benefits:\u001b[0m  Adding search capabilities boosts the accuracy for both Med-Gemini and clinician \n",
              "\u001b[1;33m   \u001b[0mperformance, highlighting the value of information retrieval in medical reasoning tasks.                        \n",
              "\n",
              "\u001b[1mImage 2: Performance Across Tasks\u001b[0m                                                                                  \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mGenerally Strong Performance:\u001b[0m While specific tasks are unnamed, Med-Gemini demonstrates superior performance    \n",
              "\u001b[1;33m   \u001b[0mcompared to the previous SOTA in most categories.                                                               \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mGPT-4 Comparison:\u001b[0m Interestingly, the best GPT-4 method lags behind both Med-Gemini and the prior SOTA in most   \n",
              "\u001b[1;33m   \u001b[0mtasks. This suggests a specialized model like Med-Gemini, fine-tuned on medical data, can outperform more       \n",
              "\u001b[1;33m   \u001b[0mgeneral-purpose language models in the medical domain.                                                          \n",
              "\n",
              "\u001b[1mImage 3: Expert Preference\u001b[0m                                                                                         \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mHigh Subjective Preference:\u001b[0m  Med-Gemini receives a high percentage of \"preferred\" responses from experts in     \n",
              "\u001b[1;33m   \u001b[0mtasks like Doctor Referral Generation and Medical Simplification. This suggests the model's outputs align well  \n",
              "\u001b[1;33m   \u001b[0mwith expert judgment in these areas.                                                                            \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mRoom for Improvement:\u001b[0m  In Medical Summarization, while Med-Gemini is still favored, there's a notable proportion\n",
              "\u001b[1;33m   \u001b[0mof \"Expert Preferred\" responses. This highlights an area where further refinement could enhance alignment with  \n",
              "\u001b[1;33m   \u001b[0mexpert preferences.                                                                                             \n",
              "\n",
              "\u001b[1mImage 4: Accuracy in Gene & DNA Tasks\u001b[0m                                                                              \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mStrengths in Complex Tasks:\u001b[0m Med-Gemini showcases strong performance in tasks like \"Protein-coding genes\" and    \n",
              "\u001b[1;33m   \u001b[0m\"Human genome TF regulation,\" demonstrating proficiency in handling intricate biological information.           \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mAreas for Attention:\u001b[0m  Tasks like \"Gene name conversion\" and \"Multi-species DNA alignment\" show a higher         \n",
              "\u001b[1;33m   \u001b[0m\"Abstain\" rate for Med-Gemini, indicating potential areas where the model may be less confident or the tasks are\n",
              "\u001b[1;33m   \u001b[0mparticularly challenging.                                                                                       \n",
              "\n",
              "\u001b[1mOverall, the images paint a picture of Med-Gemini as a high-performing medical AI model.\u001b[0m It demonstrates:          \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSuperior Accuracy:\u001b[0m Outperforming previous SOTA models and clinician baselines in challenging medical tasks.     \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mEffective Search Augmentation:\u001b[0m  Benefitting from incorporating search to improve accuracy further.              \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mExpert Alignment:\u001b[0m  Generating responses often preferred by medical experts, particularly in referral generation \n",
              "\u001b[1;33m   \u001b[0mand medical simplification.                                                                                     \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSpecialization Advantage:\u001b[0m  Outperforming even powerful general-purpose language models like GPT-4 in specific   \n",
              "\u001b[1;33m   \u001b[0mmedical domains.                                                                                                \n",
              "\n",
              "However, the analysis also reveals areas for potential improvement:                                                \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mEnhancing Summarization:\u001b[0m Refining the model to better align with expert preferences in summarization tasks.     \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mAddressing Abstentions:\u001b[0m  Investigating and potentially improving performance in tasks where the model frequently\n",
              "\u001b[1;33m   \u001b[0mabstains, suggesting lower confidence or higher task complexity.                                                \n",
              "\n",
              "In conclusion, Med-Gemini shows significant promise as a valuable tool in various medical applications. Continuous \n",
              "evaluation and refinement in areas needing improvement will further solidify its role in assisting healthcare      \n",
              "professionals and researchers.                                                                                     \n"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "instruction = f\"\"\"Answer the question and explain results with the given Image:\n",
        "Question: {query}\n",
        "Image:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare the model input\n",
        "model_input = [\n",
        "    instruction,\n",
        "    # passing all matched images to Gemini\n",
        "    \"Image:\",\n",
        "    matching_results_image[0][\"image_object\"],\n",
        "    \"Description:\",\n",
        "    matching_results_image[0][\"image_description\"],\n",
        "    \"Image:\",\n",
        "    matching_results_image[1][\"image_object\"],\n",
        "    \"Description:\",\n",
        "    matching_results_image[1][\"image_description\"],\n",
        "    \"Image:\",\n",
        "    matching_results_image[2][\"image_object\"],\n",
        "    \"Description:\",\n",
        "    matching_results_image[2][\"image_description\"],\n",
        "    \"Image:\",\n",
        "    matching_results_image[3][\"image_object\"],\n",
        "    \"Description:\",\n",
        "    matching_results_image[3][\"image_description\"],\n",
        "]\n",
        "\n",
        "# Generate Gemini response with streaming output\n",
        "rich_Markdown(\n",
        "    get_gemini_response(\n",
        "        multimodal_model_15,\n",
        "        model_input=model_input,\n",
        "        stream=True,\n",
        "        safety_settings=safety_settings,\n",
        "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uykUaQvIRNYP",
        "outputId": "846ce14c-3e0a-4254-a114-79d9c286f546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[91mCitation 1: Matched image path, page number and page text: \n",
            "\u001b[0m\n",
            "\u001b[94mscore: \u001b[0m 0.66\n",
            "\u001b[94mfile_name: \u001b[0m med_gemini.pdf\n",
            "\u001b[94mpath: \u001b[0m images/med_gemini.pdf_image_15_0_464.jpeg\n",
            "\u001b[94mpage number: \u001b[0m 16\n",
            "\u001b[94mpage text: \u001b[0m Capabilities of Gemini Models in Medicine\n",
            "(a) NEJM CPC\n",
            "(b) GeneTuring\n",
            "Figure 3 | Generalization of Med-Gemini-L 1.0 with web search to two additional text-based benchmarks. (a):\n",
            "Comparison of Med-Gemini-L 1.0s top-k accuracy on the NEJM CPC benchmark with prior SoTA LLMs and clinicians, with\n",
            "and without search. (b): Comparison between Med-Gemini-L 1.0 and SoTA models on the GeneTuring dataset modules.\n",
            "The bars represent the proportion of correct, incorrect, and abstention responses for each model.\n",
            "Revisiting MedQA (USMLE) labels\n",
            "MedQA (USMLE) is a popular benchmark for assessing the\n",
            "capabilities of LLMs in the medical domain. However, some MedQA test questions have missing\n",
            "information such as figures or lab results, and potentially outdated ground-truth answers. To address\n",
            "these concerns, we conduct a complete relabeling of the MedQA (USMLE) test set. Specifically, we\n",
            "recruit at least three US physicians to re-annotate each question, asking them to answer the question\n",
            "and evaluate the provided ground-truth answer. We also ask them to identify if there was any missing\n",
            "information in the questions. Following Stutz et al. (2023), we characterize the questions to exclude\n",
            "due to missing information or label errors by bootstrapping votes from committees of three raters per\n",
            "question. We additionally identify ambiguous questions as those allowing multiple correct answers\n",
            "(more details can be found in Appendix C.2).\n",
            "Figure 4b shows that, on average across bootstrapped committees, 3.8% of questions include\n",
            "missing information, following the unanimous vote of bootstrapped committees. Additionally, 2.9%\n",
            "likely include label errors. Another 0.7% are ambiguous. Excluding these questions is supported\n",
            "by high inter-rater agreement of 94%, 87.6%, and 94.6%, respectively. Importantly, Med-Gemini-L\n",
            "1.0s mistakes can be attributed disproportionately to these questions; our entropy-based uncertainty\n",
            "score also tends to be higher on these question (t-test, -value=0.033). Filtering both types improves\n",
            "accuracy from 91.1% to 91.8%  0.2%. Using majority instead of unanimous votes further improves\n",
            "accuracy to 92.9%  0.38% by discarding up to 20.9% of the uncertain questions.\n",
            "4.1.1. Performance on long-form medical text generation\n",
            "Med-Gemini-M 1.0 demonstrates the ability to generate long-form text for three challenging real-world\n",
            "use cases - after-visit clinical summaries, doctor referral letter generation and medical simplification.\n",
            "In side-by-side comparisons, Med-Gemini-M 1.0s responses are considered as good or better than\n",
            "expert responses more than half the time by clinician raters across the three tasks (Figure 5). For more\n",
            "task details, see Appendix C.4. Notably for the referral letter generation task, the model generated\n",
            "letters are preferred or tied with experts across all the samples evaluated.\n",
            "16\n",
            "\n",
            "\u001b[94mimage description: \u001b[0m The line chart illustrates the accuracy of different methods on the NEJM CPC task across varying Top-k values (1 to 10). \n",
            "\n",
            "Here's a breakdown:\n",
            "\n",
            "* **Y-axis:** Represents the NEJM CPC Accuracy, ranging from 10 to 70.\n",
            "* **X-axis:** Shows the \"Top-k\" values, ranging from 1 to 10.\n",
            "\n",
            "Five distinct methods are compared:\n",
            "\n",
            "* **Med-Gemini + Search (Blue triangles with line):** Shows the highest accuracy across all Top-k values, with a steady upward trend. It's surrounded by a light blue shaded area indicating a confidence interval.\n",
            "* **Med-Gemini (Blue circles with line):**  Performs slightly worse than \"Med-Gemini + Search,\" also showing an upward trend.\n",
            "* **Prior SOTA (Red circles with line):**  Has a lower accuracy compared to the \"Med-Gemini\" methods, with a gradual upward trend. It also has a light red shaded area around the line indicating a confidence interval.\n",
            "* **Clinician + Search (Orange triangles with line):** Performs better than \"Clinician\" alone, showing a slow upward trend. It has a light orange shaded area around the line.\n",
            "* **Clinician (Orange circles with line):**  Demonstrates the lowest accuracy among all methods, with a very gradual upward trend. It also has a light orange shaded area around the line.\n",
            "\n",
            "Overall, the chart suggests that both \"Med-Gemini\" methods outperform the other approaches in terms of accuracy on the NEJM CPC task. The addition of \"Search\" seems to improve the accuracy for both \"Med-Gemini\" and \"Clinician\" methods. \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3;35mNone\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## you can check the citations to probe further.\n",
        "## check the \"image description:\" which is a description extracted through Gemini which helped search our query.\n",
        "rich_print(print_text_to_image_citation(matching_results_image, print_top=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDd9rE4NrRod"
      },
      "source": [
        "## Image Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJL6ElyEy4mc"
      },
      "source": [
        "### Search similar image with image input [using multimodal image embeddings]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReKjHleFxUu9"
      },
      "source": [
        "Imagine searching for images, but instead of typing words, you use an actual image as the clue.\n",
        "\n",
        "Think of it like searching with a mini-map instead of a written address.\n",
        "It's a different way to ask, \"Show me more stuff like this\".\n",
        "\n",
        "So, instead of typing \"various example of Gemini 1.5 long context\", you show a picture of that image and say, \"Find me more like this\"\n",
        "\n",
        "For demonstration purposes, we will only be finding similar images that show the various features of Gemini in a single document below. However, you can scale this design pattern to match (find relevant images) across multiple documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DJhhS5eZw7QI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5041ca5b-d5a5-4f98-ce03-ba63a03a0431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Input image from user:***\n"
          ]
        }
      ],
      "source": [
        "# You can find a similar image as per the images you have in the metadata.\n",
        "\n",
        "image_query_path = \"images/gemini_v1_5_report_technical.pdf_image_5_0_148.jpeg\"\n",
        "\n",
        "# Print a message indicating the input image\n",
        "print(\"***Input image from user:***\")\n",
        "\n",
        "# Display the input image\n",
        "# Image.load_from_file(image_query_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zBTtGChTmrd"
      },
      "source": [
        "You expect to find images that are similar in terms of \"long context prompts for Gemini 1.5 Pro\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nZcU7vZC-8vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acda98fb-7406-426a-ca32-4d3d12799497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " **** Result: ***** \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Search for Similar Images Based on Input Image and Image Embedding\n",
        "\n",
        "matching_results_image = get_similar_image_from_query(\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    query=query,  # Use query text for additional filtering (optional)\n",
        "    column_name=\"mm_embedding_from_img_only\",  # Use image embedding for similarity calculation\n",
        "    image_emb=True,\n",
        "    image_query_path=image_query_path,  # Use input image for similarity calculation\n",
        "    top_n=3,  # Retrieve top 3 matching images\n",
        "    embedding_size=1408,  # Use embedding size of 1408\n",
        ")\n",
        "\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "# # Display the Top Matching Image\n",
        "# display(\n",
        "#     matching_results_image[0][\"image_object\"]\n",
        "# )  # Display the top matching image object (Pillow Image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhT17rke15XY"
      },
      "source": [
        "You can also print the citation to see what it has matched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mksXQoezweg0",
        "outputId": "cab7d4b5-6925-4ac0-a6e4-6056f60204c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[91mCitation 1: Matched image path, page number and page text: \n",
            "\u001b[0m\n",
            "\u001b[94mscore: \u001b[0m 0.84\n",
            "\u001b[94mfile_name: \u001b[0m gemini_v1_5_report_technical.pdf\n",
            "\u001b[94mpath: \u001b[0m images/gemini_v1_5_report_technical.pdf_image_4_1_143.jpeg\n",
            "\u001b[94mpage number: \u001b[0m 5\n",
            "\u001b[94mpage text: \u001b[0m Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\n",
            "4.1. Qualitative Examples of Multimodal Long-Context Capabilities\n",
            "The ability to process multiple millions of tokens unlocks practical applications that were not possible\n",
            "before. In this section we demonstrate some surprising interactions we observed with Gemini 1.5 Pro\n",
            "across code, text and video.\n",
            "As shown in the Figure 2, Gemini 1.5 Pro is able to ingest entire large codebases such as JAX\n",
            "(746,152 tokens), and answer very specific queries about them. in Figure 3 we show Gemini 1.5 Pros\n",
            "ability to learn a new language based only on reference materials given in its input (see Section 4.2.1.7\n",
            "for quantitative metrics for this use case). Additionally, we test Gemini 1.5 Pros ability to answer\n",
            "an image query given the entire text of Les Misrables and observe that being natively multimodal\n",
            "allows it to locate a famous scene from a hand-drawn sketch, as shown in Figure 4. Lastly, we ask\n",
            "Gemini 1.5 Pro questions about an entire movie of 45 minutes in Figure 5 which the model answers\n",
            "seamlessly while retrieving moments and timestamps down to a second. 6\n",
            "Figure 2 | Given the entire 746,152 token JAX codebase in context, Gemini 1.5 Pro can identify the\n",
            "specific location of a core automatic differentiation method.\n",
            "Figure 3 | Given a reference grammar book and a bilingual wordlist (dictionary), Gemini 1.5 Pro is\n",
            "able to translate from English to Kalamang with similar quality to a human who learned from the\n",
            "same materials.\n",
            "6For additional short videos of demonstrations of the long context abilities of Gemini 1.5 Pro across video, text, and\n",
            "code see https://deepmind.google/technologies/gemini/.\n",
            "5\n",
            "\n",
            "\u001b[94mimage description: \u001b[0m The image depicts a system designed to translate English to Kalamang. The system uses a grammar book, a dictionary, and a long context as reference materials. \n",
            "\n",
            "The user prompt is: \"Given the reference materials as context, translate the following sentence from English to Kalamang: I'm getting pandanus, I want to make a mat.\"\n",
            "\n",
            "The model output is: \"An padanual repte, irar paruotkin.\" \n",
            "\n",
            "The image also indicates that the combined grammar book and dictionary contain 250k tokens. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display citation details for the top matching image\n",
        "print_text_to_image_citation(\n",
        "    matching_results_image, print_top=True\n",
        ")  # Print citation details for the top matching image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VJWnhDJwI-uO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c609472d-4692-468d-e405-de5bbf2b0bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------Matched Images------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check Other Matched Images (Optional)\n",
        "# You can access the other two matched images using:\n",
        "\n",
        "print(\"---------------Matched Images------------------\\n\")\n",
        "# display_images(\n",
        "#     [\n",
        "#         matching_results_image[0][\"img_path\"],\n",
        "#         matching_results_image[1][\"img_path\"],\n",
        "#         matching_results_image[2][\"img_path\"],\n",
        "#     ],\n",
        "#     resize_ratio=0.2,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvwZIgD84CNc"
      },
      "source": [
        "The ability to identify similar text and images based on user input, using Gemini and embeddings, forms a crucial foundation for development of Multimodal Question Answering System with multimodal RAG design pattern, which you will explore in the coming sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUnsv5Co6pJF"
      },
      "source": [
        "### Comparative reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AFbqHiz5vvo"
      },
      "source": [
        "Next, let's apply what you have done so far in doing comparative reasoning.\n",
        "\n",
        "For this example:\n",
        "\n",
        "* **Step 1:** You will search all the images for a specific query\n",
        "\n",
        "* **Step 2:** Send those images to Gemini 1.5 Pro to ask multiple questions, where it has to compare among those images and provide you with answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6AHCSwojyX0"
      },
      "outputs": [],
      "source": [
        "matching_results_image_query_1 = get_similar_image_from_query(\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    query=\"Show me all the images that can describe LLMs and TPU v5e scaling\",\n",
        "    column_name=\"text_embedding_from_image_description\",  # Use image description text embedding # mm_embedding_from_img_only text_embedding_from_image_description\n",
        "    image_emb=False,  # Use text embedding instead of image embedding\n",
        "    top_n=5,\n",
        "    embedding_size=1408,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jja-9iGFRNYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa56a19-a793-4a22-b840-f1f379d43c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------Matched Images------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check Matched Images\n",
        "# You can access the other two matched images using:\n",
        "\n",
        "print(\"---------------Matched Images------------------\\n\")\n",
        "# display_images(\n",
        "#     [\n",
        "#         matching_results_image_query_1[0][\"img_path\"],\n",
        "#         matching_results_image_query_1[1][\"img_path\"],\n",
        "#         matching_results_image_query_1[2][\"img_path\"],\n",
        "#         matching_results_image_query_1[3][\"img_path\"],\n",
        "#         matching_results_image_query_1[4][\"img_path\"],\n",
        "#     ],\n",
        "#     resize_ratio=0.2,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSR_JWkSC_7p"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"Task: Answer the following questions in detail, providing clear reasoning and evidence from the images in bullet points.\n",
        "Instructions:\n",
        "1. Analyze the provided images focusing on the relationship between TPU v5e scaling efficiency, LLM model size growth, performance metrics, and quantization effects.\n",
        "2. Answer the following questions in detail, providing clear reasoning and evidence from the images in bullet points\n",
        "3. Cite the image sources to support your explanations. Mention the file name.\n",
        "\n",
        "Additional Considerations:\n",
        "* Clearly define any technical terms (e.g., EMFU, TFLOP/chip/s) within your answers for better understanding.\n",
        "* Use specific examples and data points from the images to support your explanations.\n",
        "* Feel free to request additional information or clarification if the images are unclear or ambiguous.\n",
        "\n",
        "Question:\n",
        " - How does the scaling efficiency of TPU v5e compare to the overall growth in LLM model size over time?\n",
        " - How does the model size impact the observed Per-chip performance and EMFU for a fixed number of TPU v5e chips (e.g., 256)?\n",
        " - For the INT8 Quant training with 32B parameters, how does its high EMFU relate to the observed TFLOP/chip/s?\n",
        " - how does the \"per device batch (seq)\" for a 16B model compare to a 128B model, and how does this affect the \"Total observed Perf\"?\n",
        " - how might the MFU be impacted by increasing LLM model size?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYkzpB4PTSfm",
        "outputId": "d201a294-6f16-4f77-ac04-a1d9a5dd5bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " **** Result: ***** \n",
            "\n",
            "CPU times: user 195 ms, sys: 39.2 ms, total: 235 ms\n",
            "Wall time: 21.6 s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "                              <span style=\"font-weight: bold; text-decoration: underline\">Analysis of TPU v5e Scaling Efficiency and LLM Training</span>                              \n",
              "\n",
              "Let's break down the provided information to understand the relationship between TPU v5e, LLM model size, and      \n",
              "performance.                                                                                                       \n",
              "\n",
              "<span style=\"font-weight: bold\">Terminology:</span>                                                                                                       \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">EMFU:</span> Exa Multiply-accumulate operations per second Full Utilization. This metric represents the theoretical    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>peak performance of the TPU system when fully utilized.                                                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">TFLOP/chip/s:</span> Tera Floating Point Operations per second per chip. This metric indicates the actual computational\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>throughput of each TPU chip.                                                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">INT8 Quant:</span> INT8 Quantization. This technique reduces the precision of model weights and activations from higher\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>precision formats like BF16 (brain float 16) to INT8 (8-bit integer), reducing memory footprint and             \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>computational cost.                                                                                             \n",
              "\n",
              "<span style=\"font-weight: bold\">Answers:</span>                                                                                                           \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">Scaling Efficiency vs. LLM Model Size Growth:</span>                                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Observation:</span> \"TPU v5e Efficient Scaling with 32B LLM\" (image 1) shows the scaling efficiency of TPU v5e      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>decreases as the number of chips increases.                                                                  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Reasoning:</span> While larger TPU systems provide more computational power, the communication overhead and data    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>movement between chips become bottlenecks, reducing overall efficiency. This trend is common in large-scale  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>distributed computing.                                                                                       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">LLM Growth:</span> \"LLM model size growth\" (image 2) illustrates the exponential growth of LLM model sizes over     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>time.                                                                                                        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Comparison:</span> The declining scaling efficiency of TPUs highlights the challenge of efficiently training        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>increasingly larger LLMs. Even with hardware advancements, maximizing performance requires careful           \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>optimization and potentially novel architectural approaches.                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Model Size Impact on Performance (256 Chips):</span>                                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Per-chip Performance:</span> \"MaxText LLM Training Results\" (image 3) shows a decrease in \"Per-chip\" performance    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>with increasing model size for a fixed 256 TPU v5e chips. For instance, the Per-chip performance drops from  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>132 TFLOP/s (32B model) to 110 TFLOP/s (128B model).                                                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Reasoning:</span> Larger models require more data movement and communication, exceeding the memory bandwidth and    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>inter-chip communication capabilities of the system. This bottleneck leads to reduced per-chip utilization   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>and lower performance.                                                                                       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">EMFU:</span> The EMFU generally decreases with increasing model size. For example, EMFU drops from 66.86% (32B      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>model) to 56.06% (128B model) on 256 chips.                                                                  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Reasoning:</span> This trend reinforces the challenge of keeping all TPU cores fully utilized with larger models.   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>The increased communication overhead and memory bottlenecks prevent achieving maximum theoretical            \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>performance.                                                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">INT8 Quant, EMFU, and TFLOP/chip/s (32B Model):</span>                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">High EMFU:</span> Image 3 shows that INT8 quantization with a 32B model on 199 TPU v5e pods (50944 chips) achieves a\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>high EMFU of 52.99%.                                                                                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">TFLOP/chip/s:</span> This configuration also achieves 104.4 TOP/s per chip.                                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Relationship:</span> Quantization reduces the computational complexity and memory footprint of the model. This      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>reduction allows for better utilization of the TPU cores, leading to higher EMFU and TFLOP/chip/s compared to\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>BF16 training with the same number of chips.                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">Per-Device Batch Size and Total Performance:</span>                                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Batch Size Comparison:</span> \"MaxText Model Configurations\" (image 5) shows that the \"per device batch (seq)\"      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>decreases with increasing model size. A 16B model uses a batch size of 6, while a 128B model uses only 1.    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Reasoning:</span> Larger models require more memory. To accommodate these models within the limited memory capacity \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>of each TPU, the batch size needs to be reduced.                                                             \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Impact on Total Performance:</span> Smaller batch sizes generally lead to slower training convergence and can       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>potentially reduce the \"Total observed Perf\" despite the higher computational power of larger TPU systems.   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 5 </span><span style=\"font-weight: bold\">MFU and Increasing LLM Model Size:</span>                                                                              \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">MFU (Matrix Multiply Unit):</span>  MFUs are specialized hardware units within TPUs optimized for performing matrix \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>multiplications, which are core operations in LLMs.                                                          \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Impact:</span> As LLM model sizes grow, the computational demands on the MFUs increase significantly. This can lead \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>to MFU utilization becoming a bottleneck, especially if the memory bandwidth and inter-chip communication    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>cannot keep up with the required data supply rate.                                                           \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Potential Mitigation:</span> Addressing this bottleneck might involve architectural changes to the TPU, such as     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>increasing on-chip memory, improving memory bandwidth, or exploring new interconnect technologies for faster \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>data movement between chips.                                                                                 \n",
              "\n",
              "<span style=\"font-weight: bold\">Conclusion:</span>                                                                                                        \n",
              "\n",
              "The insights derived from the provided images highlight the complex interplay between hardware capabilities, model \n",
              "characteristics, and software optimizations in LLM training. While TPU v5e demonstrates impressive performance,    \n",
              "efficiently scaling training for increasingly larger LLM models presents ongoing challenges that necessitate       \n",
              "continuous innovation in hardware and software solutions.                                                          \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "                              \u001b[1;4mAnalysis of TPU v5e Scaling Efficiency and LLM Training\u001b[0m                              \n",
              "\n",
              "Let's break down the provided information to understand the relationship between TPU v5e, LLM model size, and      \n",
              "performance.                                                                                                       \n",
              "\n",
              "\u001b[1mTerminology:\u001b[0m                                                                                                       \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mEMFU:\u001b[0m Exa Multiply-accumulate operations per second Full Utilization. This metric represents the theoretical    \n",
              "\u001b[1;33m   \u001b[0mpeak performance of the TPU system when fully utilized.                                                         \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mTFLOP/chip/s:\u001b[0m Tera Floating Point Operations per second per chip. This metric indicates the actual computational\n",
              "\u001b[1;33m   \u001b[0mthroughput of each TPU chip.                                                                                    \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mINT8 Quant:\u001b[0m INT8 Quantization. This technique reduces the precision of model weights and activations from higher\n",
              "\u001b[1;33m   \u001b[0mprecision formats like BF16 (brain float 16) to INT8 (8-bit integer), reducing memory footprint and             \n",
              "\u001b[1;33m   \u001b[0mcomputational cost.                                                                                             \n",
              "\n",
              "\u001b[1mAnswers:\u001b[0m                                                                                                           \n",
              "\n",
              "\u001b[1;33m 1 \u001b[0m\u001b[1mScaling Efficiency vs. LLM Model Size Growth:\u001b[0m                                                                   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mObservation:\u001b[0m \"TPU v5e Efficient Scaling with 32B LLM\" (image 1) shows the scaling efficiency of TPU v5e      \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mdecreases as the number of chips increases.                                                                  \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mReasoning:\u001b[0m While larger TPU systems provide more computational power, the communication overhead and data    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mmovement between chips become bottlenecks, reducing overall efficiency. This trend is common in large-scale  \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mdistributed computing.                                                                                       \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mLLM Growth:\u001b[0m \"LLM model size growth\" (image 2) illustrates the exponential growth of LLM model sizes over     \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtime.                                                                                                        \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mComparison:\u001b[0m The declining scaling efficiency of TPUs highlights the challenge of efficiently training        \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mincreasingly larger LLMs. Even with hardware advancements, maximizing performance requires careful           \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0moptimization and potentially novel architectural approaches.                                                 \n",
              "\u001b[1;33m 2 \u001b[0m\u001b[1mModel Size Impact on Performance (256 Chips):\u001b[0m                                                                   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mPer-chip Performance:\u001b[0m \"MaxText LLM Training Results\" (image 3) shows a decrease in \"Per-chip\" performance    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mwith increasing model size for a fixed 256 TPU v5e chips. For instance, the Per-chip performance drops from  \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m132 TFLOP/s (32B model) to 110 TFLOP/s (128B model).                                                         \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mReasoning:\u001b[0m Larger models require more data movement and communication, exceeding the memory bandwidth and    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0minter-chip communication capabilities of the system. This bottleneck leads to reduced per-chip utilization   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mand lower performance.                                                                                       \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mEMFU:\u001b[0m The EMFU generally decreases with increasing model size. For example, EMFU drops from 66.86% (32B      \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mmodel) to 56.06% (128B model) on 256 chips.                                                                  \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mReasoning:\u001b[0m This trend reinforces the challenge of keeping all TPU cores fully utilized with larger models.   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mThe increased communication overhead and memory bottlenecks prevent achieving maximum theoretical            \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mperformance.                                                                                                 \n",
              "\u001b[1;33m 3 \u001b[0m\u001b[1mINT8 Quant, EMFU, and TFLOP/chip/s (32B Model):\u001b[0m                                                                 \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mHigh EMFU:\u001b[0m Image 3 shows that INT8 quantization with a 32B model on 199 TPU v5e pods (50944 chips) achieves a\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mhigh EMFU of 52.99%.                                                                                         \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mTFLOP/chip/s:\u001b[0m This configuration also achieves 104.4 TOP/s per chip.                                         \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mRelationship:\u001b[0m Quantization reduces the computational complexity and memory footprint of the model. This      \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mreduction allows for better utilization of the TPU cores, leading to higher EMFU and TFLOP/chip/s compared to\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mBF16 training with the same number of chips.                                                                 \n",
              "\u001b[1;33m 4 \u001b[0m\u001b[1mPer-Device Batch Size and Total Performance:\u001b[0m                                                                    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mBatch Size Comparison:\u001b[0m \"MaxText Model Configurations\" (image 5) shows that the \"per device batch (seq)\"      \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mdecreases with increasing model size. A 16B model uses a batch size of 6, while a 128B model uses only 1.    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mReasoning:\u001b[0m Larger models require more memory. To accommodate these models within the limited memory capacity \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mof each TPU, the batch size needs to be reduced.                                                             \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mImpact on Total Performance:\u001b[0m Smaller batch sizes generally lead to slower training convergence and can       \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mpotentially reduce the \"Total observed Perf\" despite the higher computational power of larger TPU systems.   \n",
              "\u001b[1;33m 5 \u001b[0m\u001b[1mMFU and Increasing LLM Model Size:\u001b[0m                                                                              \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mMFU (Matrix Multiply Unit):\u001b[0m  MFUs are specialized hardware units within TPUs optimized for performing matrix \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mmultiplications, which are core operations in LLMs.                                                          \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mImpact:\u001b[0m As LLM model sizes grow, the computational demands on the MFUs increase significantly. This can lead \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mto MFU utilization becoming a bottleneck, especially if the memory bandwidth and inter-chip communication    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mcannot keep up with the required data supply rate.                                                           \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mPotential Mitigation:\u001b[0m Addressing this bottleneck might involve architectural changes to the TPU, such as     \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mincreasing on-chip memory, improving memory bandwidth, or exploring new interconnect technologies for faster \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mdata movement between chips.                                                                                 \n",
              "\n",
              "\u001b[1mConclusion:\u001b[0m                                                                                                        \n",
              "\n",
              "The insights derived from the provided images highlight the complex interplay between hardware capabilities, model \n",
              "characteristics, and software optimizations in LLM training. While TPU v5e demonstrates impressive performance,    \n",
              "efficiently scaling training for increasingly larger LLM models presents ongoing challenges that necessitate       \n",
              "continuous innovation in hardware and software solutions.                                                          \n"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# Generate response with Gemini 1.5 Pro\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "rich_Markdown(\n",
        "    get_gemini_response(\n",
        "        multimodal_model_15,\n",
        "        model_input=[\n",
        "            prompt,\n",
        "            \"Images:\",\n",
        "            matching_results_image_query_1[0][\"image_object\"],\n",
        "            matching_results_image_query_1[1][\"image_object\"],\n",
        "            matching_results_image_query_1[2][\"image_object\"],\n",
        "            matching_results_image_query_1[3][\"image_object\"],\n",
        "            matching_results_image_query_1[4][\"image_object\"],\n",
        "        ],\n",
        "        stream=True,\n",
        "        safety_settings=safety_settings,\n",
        "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efJPPrzRhvIT"
      },
      "source": [
        "## Building Multimodal QA System with retrieval augmented generation (mRAG)\n",
        "\n",
        "Let's bring everything together to implement multimodal RAG. You will use all the elements that you've explored in previous sections to implement the multimodal RAG. These are the steps:\n",
        "\n",
        "* **Step 1:** The user gives a query in text format where the expected information is available in the document and is embedded in images and text.\n",
        "* **Step 2:** Find all text chunks from the pages in the documents using a method similar to the one you explored in `Text Search`.\n",
        "* **Step 3:** Find all similar images from the pages based on the user query matched with `image_description` using a method identical to the one you explored in `Image Search`.\n",
        "* **Step 4:** Combine all similar text and images found in steps 2 and 3 as `context_text` and `context_images`.\n",
        "* **Step 5:** With the help of Gemini, we can pass the user query with text and image context found in steps 2 & 3. You can also add a specific instruction the model should remember while answering the user query.\n",
        "* **Step 6:** Gemini produces the answer, and you can print the citations to check all relevant text and images used to address the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI62Hzuw_0_b"
      },
      "source": [
        "### Step 1: User query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvTKFwOPHLQ_"
      },
      "outputs": [],
      "source": [
        "# this time we are not passing any images, but just a simple text query.\n",
        "\n",
        "query = \"\"\"- How does the scaling efficiency of TPU v5e compare to the overall growth in LLM model size over time?\n",
        " - How does the model size impact the observed Per-chip performance and EMFU for a fixed number of TPU v5e chips (e.g., 256)?\n",
        " - For the INT8 Quant training with 32B parameters, how does its high EMFU relate to the observed TFLOP/chip/s?\n",
        " - how does the \"per device batch (seq)\" for a 16B model compare to a 128B model, and how does this affect the \"Total observed Perf\"?\n",
        " - how might the MFU be impacted by increasing LLM model size?\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUqlkKUaYvZA"
      },
      "source": [
        "### Step 2: Get all relevant text chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r65yBb5gR_NG"
      },
      "outputs": [],
      "source": [
        "# Retrieve relevant chunks of text based on the query\n",
        "matching_results_chunks_data = get_similar_text_from_query(\n",
        "    query,\n",
        "    text_metadata_df,\n",
        "    column_name=\"text_embedding_chunk\",\n",
        "    top_n=20,\n",
        "    chunk_text=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIgXgVIpYzxj"
      },
      "source": [
        "### Step 3: Get all relevant images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzu5Gf4yR_J4"
      },
      "outputs": [],
      "source": [
        "# Get all relevant images based on user query\n",
        "matching_results_image_fromdescription_data = get_similar_image_from_query(\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    query=query,\n",
        "    column_name=\"text_embedding_from_image_description\",\n",
        "    image_emb=False,\n",
        "    top_n=10,\n",
        "    embedding_size=1408,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhUpWlGAY2uG"
      },
      "source": [
        "### Step 4: Create context_text and context_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_EEuuLCe6Y5"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"Task: Answer the following questions in detail, providing clear reasoning and evidence from the images and text in bullet points.\n",
        "Instructions:\n",
        "\n",
        "1. **Analyze:** Carefully examine the provided images and text context.\n",
        "2. **Synthesize:** Integrate information from both the visual and textual elements.\n",
        "3. **Reason:**  Deduce logical connections and inferences to address the question.\n",
        "4. **Respond:** Provide a concise, accurate answer in the following format:\n",
        "\n",
        "   * **Question:** [Question]\n",
        "   * **Answer:** [Direct response to the question]\n",
        "   * **Explanation:** [Bullet-point reasoning steps if applicable]\n",
        "   * **Source** [name of the file, page, image from where the information is citied]\n",
        "\n",
        "5. **Ambiguity:** If the context is insufficient to answer, respond \"Not enough context to answer.\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# combine all the selected relevant text chunks\n",
        "context_text = [\"Text Context: \"]\n",
        "for key, value in matching_results_chunks_data.items():\n",
        "    context_text.extend(\n",
        "        [\n",
        "            \"Text Source: \",\n",
        "            f\"\"\"file_name: \"{value[\"file_name\"]}\" Page: \"{value[\"page_num\"]}\"\"\",\n",
        "            \"Text\",\n",
        "            value[\"chunk_text\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# combine all the selected relevant images\n",
        "gemini_content = [\n",
        "    instruction,\n",
        "    \"Questions: \",\n",
        "    query,\n",
        "    \"Image Context: \",\n",
        "]\n",
        "for key, value in matching_results_image_fromdescription_data.items():\n",
        "    gemini_content.extend(\n",
        "        [\n",
        "            \"Image Path: \",\n",
        "            value[\"img_path\"],\n",
        "            \"Image Description: \",\n",
        "            value[\"image_description\"],\n",
        "            \"Image:\",\n",
        "            value[\"image_object\"],\n",
        "        ]\n",
        "    )\n",
        "gemini_content.extend(context_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHrtodcBAEu9"
      },
      "source": [
        "### Step 5: Pass context to Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZuhtJu7fW4n",
        "outputId": "37acab63-5d0a-4402-9320-29f71e36e6b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question:</span> How does the scaling efficiency of TPU v5e compare to the overall growth in LLM model size over time? \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Answer:</span> Not enough context to answer.                                                                           \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Explanation:</span>                                                                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>While the text mentions the scaling efficiency of TPU v5e and provides data on its performance with different\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>MaxText LLM sizes, it doesn't offer a direct comparison to the overall growth in LLM model size over time.   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>The \"LLM model size growth\" chart shows the trend of increasing model size, but we need more specific data on\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>TPU v5e's scaling efficiency at each point in time to establish a correlation.                               \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question:</span> How does the model size impact the observed Per-chip performance and EMFU for a fixed number of TPU   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>v5e chips (e.g., 256)?                                                                                          \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Answer:</span> For a fixed number of TPU v5e chips (256), increasing the model size generally leads to a decrease in   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>\"Observed Per-chip performance\" but doesn't show a clear trend for EMFU.                                        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Explanation:</span>                                                                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Observing the \"MaxText LLM Training Results\" table, specifically the rows with 256 TPU v5e chips, as model   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>size increases from 16B to 128B parameters, the \"Observed Perchip Perf\" decreases (120 TFLOP/s to 110        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>TFLOP/s).                                                                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>However, EMFU fluctuates without a clear trend. It starts at 61.10% for 16B, peaks at 66.86% for 32B, then   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>decreases to 56.06% for 128B.                                                                                \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Source:</span> images/Google Cloud TPU blog.pdf_image_13_0_69.jpeg                                                     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question:</span> For the INT8 Quant training with 32B parameters, how does its high EMFU relate to the observed        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>TFLOP/chip/s?                                                                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Answer:</span> Not enough context to answer.                                                                           \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Explanation:</span>                                                                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>The table provides the EMFU (52.99%) for the INT8 Quant training with 32B parameters.                        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>However, it doesn't list the observed TOP/chip/s, which is necessary to calculate EMFU based on the formula: \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>EMFU = (observed TOP/chip/s) / (peak hardware TFLOP/chip/s).                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Without the observed TOP/chip/s, we can't establish a direct relationship between EMFU and the observed      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>TFLOP/chip/s.                                                                                                \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Source:</span> images/Google Cloud TPU blog.pdf_image_13_0_69.jpeg and images/Google Cloud TPU                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>blog.pdf_image_12_0_66.jpeg                                                                                     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question:</span> how does the \"per device batch (seq)\" for a 16B model compare to a 128B model, and how does this      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>affect the \"Total observed Perf\"?                                                                               \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Answer:</span>  The \"per device batch (seq)\" for a 16B model (6) is six times larger than a 128B model (1). This       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>difference in batch size likely contributes to the 16B model having a lower \"Total observed Perf\" compared to   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>the 128B model, despite having higher \"Observed Perchip Perf\".                                                  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Explanation:</span>                                                                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>The \"MaxText Model Configurations\" table shows that the \"per device batch (seq)\" decreases as the model size \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>increases.                                                                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>A smaller batch size for larger models is expected, as they require more memory per instance.                \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>While the 16B model has a higher \"Observed Perchip Perf\" (120 TFLOP/s vs. 110 TFLOP/s) with 256 chips, its   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>smaller total batch size (6 vs. 1) likely limits its overall throughput, resulting in a lower \"Total observed\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>Perf\" (0.03 exa-FLOPs vs. 0.03 exa-FLOPs).                                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Source:</span> images/Google Cloud TPU blog.pdf_image_13_0_69.jpeg and images/Google Cloud TPU                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>blog.pdf_image_6_0_35.jpeg                                                                                      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question:</span> How might the MFU be impacted by increasing LLM model size?                                           \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Answer:</span> Based on the provided information, we can infer that increasing LLM model size might lead to a decrease \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>in MFU.                                                                                                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Explanation:</span>                                                                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Larger models often require more complex communication patterns and memory management, potentially leading to\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>increased overhead and reduced computational efficiency.                                                     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>The \"MaxText LLM Training Results\" table shows that for a fixed number of chips (256), \"Observed Perchip     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>Perf\" generally decreases as the model size increases. Since MFU is calculated using \"observed TFLOP/chip/s\",\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>a decrease in \"Observed Perchip Perf\" generally indicates a potential decrease in MFU.                       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Source:</span> images/Google Cloud TPU blog.pdf_image_13_0_69.jpeg and images/Google Cloud TPU                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>blog.pdf_image_11_1_63.jpeg                                                                                     \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mQuestion:\u001b[0m How does the scaling efficiency of TPU v5e compare to the overall growth in LLM model size over time? \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mAnswer:\u001b[0m Not enough context to answer.                                                                           \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mExplanation:\u001b[0m                                                                                                    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mWhile the text mentions the scaling efficiency of TPU v5e and provides data on its performance with different\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mMaxText LLM sizes, it doesn't offer a direct comparison to the overall growth in LLM model size over time.   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mThe \"LLM model size growth\" chart shows the trend of increasing model size, but we need more specific data on\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mTPU v5e's scaling efficiency at each point in time to establish a correlation.                               \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mQuestion:\u001b[0m How does the model size impact the observed Per-chip performance and EMFU for a fixed number of TPU   \n",
              "\u001b[1;33m   \u001b[0mv5e chips (e.g., 256)?                                                                                          \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mAnswer:\u001b[0m For a fixed number of TPU v5e chips (256), increasing the model size generally leads to a decrease in   \n",
              "\u001b[1;33m   \u001b[0m\"Observed Per-chip performance\" but doesn't show a clear trend for EMFU.                                        \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mExplanation:\u001b[0m                                                                                                    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mObserving the \"MaxText LLM Training Results\" table, specifically the rows with 256 TPU v5e chips, as model   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0msize increases from 16B to 128B parameters, the \"Observed Perchip Perf\" decreases (120 TFLOP/s to 110        \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mTFLOP/s).                                                                                                    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mHowever, EMFU fluctuates without a clear trend. It starts at 61.10% for 16B, peaks at 66.86% for 32B, then   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mdecreases to 56.06% for 128B.                                                                                \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSource:\u001b[0m images/Google Cloud TPU blog.pdf_image_13_0_69.jpeg                                                     \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mQuestion:\u001b[0m For the INT8 Quant training with 32B parameters, how does its high EMFU relate to the observed        \n",
              "\u001b[1;33m   \u001b[0mTFLOP/chip/s?                                                                                                   \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mAnswer:\u001b[0m Not enough context to answer.                                                                           \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mExplanation:\u001b[0m                                                                                                    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mThe table provides the EMFU (52.99%) for the INT8 Quant training with 32B parameters.                        \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mHowever, it doesn't list the observed TOP/chip/s, which is necessary to calculate EMFU based on the formula: \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mEMFU = (observed TOP/chip/s) / (peak hardware TFLOP/chip/s).                                                 \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mWithout the observed TOP/chip/s, we can't establish a direct relationship between EMFU and the observed      \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mTFLOP/chip/s.                                                                                                \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSource:\u001b[0m images/Google Cloud TPU blog.pdf_image_13_0_69.jpeg and images/Google Cloud TPU                         \n",
              "\u001b[1;33m   \u001b[0mblog.pdf_image_12_0_66.jpeg                                                                                     \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mQuestion:\u001b[0m how does the \"per device batch (seq)\" for a 16B model compare to a 128B model, and how does this      \n",
              "\u001b[1;33m   \u001b[0maffect the \"Total observed Perf\"?                                                                               \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mAnswer:\u001b[0m  The \"per device batch (seq)\" for a 16B model (6) is six times larger than a 128B model (1). This       \n",
              "\u001b[1;33m   \u001b[0mdifference in batch size likely contributes to the 16B model having a lower \"Total observed Perf\" compared to   \n",
              "\u001b[1;33m   \u001b[0mthe 128B model, despite having higher \"Observed Perchip Perf\".                                                  \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mExplanation:\u001b[0m                                                                                                    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mThe \"MaxText Model Configurations\" table shows that the \"per device batch (seq)\" decreases as the model size \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mincreases.                                                                                                   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mA smaller batch size for larger models is expected, as they require more memory per instance.                \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mWhile the 16B model has a higher \"Observed Perchip Perf\" (120 TFLOP/s vs. 110 TFLOP/s) with 256 chips, its   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0msmaller total batch size (6 vs. 1) likely limits its overall throughput, resulting in a lower \"Total observed\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mPerf\" (0.03 exa-FLOPs vs. 0.03 exa-FLOPs).                                                                   \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSource:\u001b[0m images/Google Cloud TPU blog.pdf_image_13_0_69.jpeg and images/Google Cloud TPU                         \n",
              "\u001b[1;33m   \u001b[0mblog.pdf_image_6_0_35.jpeg                                                                                      \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mQuestion:\u001b[0m How might the MFU be impacted by increasing LLM model size?                                           \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mAnswer:\u001b[0m Based on the provided information, we can infer that increasing LLM model size might lead to a decrease \n",
              "\u001b[1;33m   \u001b[0min MFU.                                                                                                         \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mExplanation:\u001b[0m                                                                                                    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mLarger models often require more complex communication patterns and memory management, potentially leading to\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mincreased overhead and reduced computational efficiency.                                                     \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mThe \"MaxText LLM Training Results\" table shows that for a fixed number of chips (256), \"Observed Perchip     \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mPerf\" generally decreases as the model size increases. Since MFU is calculated using \"observed TFLOP/chip/s\",\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0ma decrease in \"Observed Perchip Perf\" generally indicates a potential decrease in MFU.                       \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSource:\u001b[0m images/Google Cloud TPU blog.pdf_image_13_0_69.jpeg and images/Google Cloud TPU                         \n",
              "\u001b[1;33m   \u001b[0mblog.pdf_image_11_1_63.jpeg                                                                                     \n"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate Gemini response with streaming output\n",
        "rich_Markdown(\n",
        "    get_gemini_response(\n",
        "        multimodal_model_15,\n",
        "        model_input=gemini_content,\n",
        "        stream=True,\n",
        "        safety_settings=safety_settings,\n",
        "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0FtXYl1fzKh"
      },
      "source": [
        "### Step 6: Print citations and references [Optional]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voThgteH-Tm8"
      },
      "source": [
        "**Optional:** Uncomment to see the detailed citations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYRLQ47or1I8"
      },
      "outputs": [],
      "source": [
        "# print(\"---------------Matched Images------------------\\n\")\n",
        "# display_images(\n",
        "#     [\n",
        "#         matching_results_image_fromdescription_data[0][\"img_path\"],\n",
        "#         matching_results_image_fromdescription_data[1][\"img_path\"],\n",
        "#     ],\n",
        "#     resize_ratio=0.2,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buwd_gp6HJ5K"
      },
      "outputs": [],
      "source": [
        "# # Image citations. You can check how Gemini generated metadata helped in grounding the answer.\n",
        "\n",
        "# print_text_to_image_citation(\n",
        "#     matching_results_image_fromdescription_data, print_top=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06vYM4MOHJ1-"
      },
      "outputs": [],
      "source": [
        "# # Text citations\n",
        "\n",
        "# print_text_to_text_citation(\n",
        "#     matching_results_chunks_data,\n",
        "#     print_top=True,\n",
        "#     chunk_text=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIVF-QHuGVDD"
      },
      "source": [
        "### Multimodal RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U82wS4nIB8IS"
      },
      "source": [
        "### More questions with Multimodal QA System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAyQVQ54B8X0",
        "outputId": "bd1624f0-1b54-459d-f14b-e2189a580468"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "                                               <span style=\"font-weight: bold; text-decoration: underline\">Med-Gemini Analysis:</span>                                                \n",
              "\n",
              "<span style=\"font-weight: bold\">Question 1:</span> Imagine a patient presents with new-onset prurigo nodularis. Could Med-Gemini-M 1.5 be used to analyze \n",
              "dermatological images of the patient's lesions in conjunction with a comprehensive history taken from an EHR       \n",
              "dialogue to help a clinician reach a diagnosis and develop a treatment plan? What are the limitations and potential\n",
              "ethical considerations of using the model in this way?                                                             \n",
              "\n",
              "<span style=\"font-weight: bold\">Answer:</span> While Med-Gemini-M 1.5 shows promise for multimodal diagnosis, it's not ready for real-world deployment in \n",
              "its current form.                                                                                                  \n",
              "\n",
              "<span style=\"font-weight: bold\">Explanation:</span>                                                                                                       \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Potential:</span>                                                                                                      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Multimodal Integration:</span> Med-Gemini-M 1.5 demonstrates the ability to analyze both text (EHR dialogues) and   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>images (dermatological photos), potentially aiding in prurigo nodularis diagnosis.                           \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Diagnosis &amp; Treatment Suggestions:</span>  Figure 6a showcases a hypothetical dialogue where Med-Gemini-M 1.5       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>accurately diagnoses prurigo nodularis from a patient description and image, and provides treatment options. \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"font-weight: bold\">Source: med_gemini.pdf, Page 18, 19, Figure 6a</span>                                                               \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Limitations:</span>                                                                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Limited Data:</span> Figure 6b highlights that the example relies on \"limited data\" of one photo and brief          \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>description. More comprehensive data is needed for robust diagnosis. <span style=\"font-weight: bold\">Source: med_gemini.pdf, Page 19, Figure </span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"font-weight: bold\">6b</span>                                                                                                           \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Lack of Real-world Validation:</span> The paper acknowledges the need for \"considerable further research and        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>development\" before real-world use. <span style=\"font-weight: bold\">Source: med_gemini.pdf, Page 19</span>                                          \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Ethical Considerations:</span>                                                                                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Bias:</span>  The model's training data could contain biases, leading to inaccurate or unfair diagnoses for certain \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>demographics.                                                                                                \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Over-reliance:</span> Clinicians might over-rely on the model's output, potentially missing crucial nuances in      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>patient history or symptoms.                                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Patient Privacy:</span>  Integrating patient data raises privacy concerns, requiring robust data security and       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>informed consent protocols.                                                                                  \n",
              "\n",
              "<span style=\"font-weight: bold\">Source:</span> med_gemini.pdf, Pages 18-19, Figure 6                                                                      \n",
              "\n",
              "<span style=\"font-weight: bold\">Question 2:</span> The paper focuses on uncertainty-guided search for text-based reasoning tasks. How could this approach \n",
              "be extended to multimodal tasks? For instance, if Med-Gemini-M 1.5 encounters uncertainty when analyzing a         \n",
              "dermatology image, could it generate queries to search for relevant visual examples or supplemental clinical       \n",
              "information to refine its interpretation?                                                                          \n",
              "\n",
              "<span style=\"font-weight: bold\">Answer:</span> Yes, uncertainty-guided search can be extended to multimodal tasks.  Med-Gemini-M 1.5 could generate       \n",
              "queries for both visual and textual information to reduce uncertainty.                                             \n",
              "\n",
              "<span style=\"font-weight: bold\">Explanation:</span>                                                                                                       \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Current Approach:</span>  The paper describes using uncertainty-guided search for text-based reasoning. When the model \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>is uncertain, it generates search queries to gather more information. <span style=\"font-weight: bold\">Source: med_gemini.pdf, Page 9</span>            \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Multimodal Extension:</span> This approach can be adapted for multimodal uncertainty:                                  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Visual Queries:</span>  If uncertain about a dermatology image, Med-Gemini-M 1.5 could generate queries like        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>\"prurigo nodularis different skin tones\" or \"dermatofibroma vs. prurigo nodularis images\" to retrieve        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>relevant visual examples.                                                                                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Supplemental Information Queries:</span> It could also generate queries for text-based information, such as \"prurigo\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>nodularis histopathology findings\" or \"differential diagnosis of prurigo nodularis.\"                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Refined Interpretation:</span> By accessing and integrating both visual and textual search results, the model could    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>improve its understanding and refine its interpretation of the image.                                           \n",
              "\n",
              "<span style=\"font-weight: bold\">Source:</span> med_gemini.pdf, Page 9                                                                                     \n",
              "\n",
              "<span style=\"font-weight: bold\">Question 3:</span> Considering the potential benefits and risks highlighted in the paper, what specific steps should be   \n",
              "taken during the development, validation, and deployment of Med-Gemini models to ensure they are used safely,      \n",
              "fairly, and effectively in real-world clinical settings? How can these steps be informed by ongoing collaboration  \n",
              "between researchers, clinicians, regulators, and patient communities?                                              \n",
              "\n",
              "<span style=\"font-weight: bold\">Answer:</span> Ensuring safe, fair, and effective use of Med-Gemini models requires a multi-faceted approach throughout   \n",
              "their lifecycle:                                                                                                   \n",
              "\n",
              "<span style=\"font-weight: bold\">Development:</span>                                                                                                       \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Diverse &amp; Representative Data:</span>  Use training data that reflects diverse patient demographics and clinical       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>presentations to mitigate bias. <span style=\"font-weight: bold\">Collaboration:</span> Engage with patient communities to identify potential data gaps  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>and biases.                                                                                                     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Transparency &amp; Explainability:</span> Develop methods to make the model's reasoning transparent and understandable to  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>clinicians. <span style=\"font-weight: bold\">Collaboration:</span> Involve clinicians to determine the level of explainability needed for trust and     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>effective use.                                                                                                  \n",
              "\n",
              "<span style=\"font-weight: bold\">Validation:</span>                                                                                                        \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Rigorous Testing:</span> Conduct extensive testing on diverse datasets and in real-world clinical settings to assess   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>accuracy, safety, and fairness. <span style=\"font-weight: bold\">Collaboration:</span> Work with regulators to design appropriate validation studies    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>that meet regulatory standards.                                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Bias Audits:</span>  Regularly audit the model for biases and implement mitigation strategies. <span style=\"font-weight: bold\">Collaboration:</span>          \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Collaborate with ethicists and social scientists to develop comprehensive bias detection methods.               \n",
              "\n",
              "<span style=\"font-weight: bold\">Deployment:</span>                                                                                                        \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Human Oversight:</span> Implement systems that ensure human clinicians make final decisions and maintain control over  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>patient care. <span style=\"font-weight: bold\">Collaboration:</span> Work with clinicians to define clear guidelines for human oversight and            \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>intervention.                                                                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Continuous Monitoring:</span> Monitor the model's performance in real-world settings and make adjustments as needed.   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"font-weight: bold\">Collaboration:</span> Establish feedback mechanisms for clinicians and patients to report issues and contribute to     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>model improvement.                                                                                              \n",
              "\n",
              "<span style=\"font-weight: bold\">Ongoing Collaboration:</span>  Regular communication and collaboration between researchers, clinicians, regulators, and   \n",
              "patient communities are crucial throughout the entire process to ensure responsible and beneficial use of          \n",
              "Med-Gemini models.                                                                                                 \n",
              "\n",
              "<span style=\"font-weight: bold\">Source:</span>  med_gemini.pdf (general principles throughout the paper)                                                  \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "                                               \u001b[1;4mMed-Gemini Analysis:\u001b[0m                                                \n",
              "\n",
              "\u001b[1mQuestion 1:\u001b[0m Imagine a patient presents with new-onset prurigo nodularis. Could Med-Gemini-M 1.5 be used to analyze \n",
              "dermatological images of the patient's lesions in conjunction with a comprehensive history taken from an EHR       \n",
              "dialogue to help a clinician reach a diagnosis and develop a treatment plan? What are the limitations and potential\n",
              "ethical considerations of using the model in this way?                                                             \n",
              "\n",
              "\u001b[1mAnswer:\u001b[0m While Med-Gemini-M 1.5 shows promise for multimodal diagnosis, it's not ready for real-world deployment in \n",
              "its current form.                                                                                                  \n",
              "\n",
              "\u001b[1mExplanation:\u001b[0m                                                                                                       \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mPotential:\u001b[0m                                                                                                      \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mMultimodal Integration:\u001b[0m Med-Gemini-M 1.5 demonstrates the ability to analyze both text (EHR dialogues) and   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mimages (dermatological photos), potentially aiding in prurigo nodularis diagnosis.                           \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mDiagnosis & Treatment Suggestions:\u001b[0m  Figure 6a showcases a hypothetical dialogue where Med-Gemini-M 1.5       \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0maccurately diagnoses prurigo nodularis from a patient description and image, and provides treatment options. \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1mSource: med_gemini.pdf, Page 18, 19, Figure 6a\u001b[0m                                                               \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mLimitations:\u001b[0m                                                                                                    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mLimited Data:\u001b[0m Figure 6b highlights that the example relies on \"limited data\" of one photo and brief          \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mdescription. More comprehensive data is needed for robust diagnosis. \u001b[1mSource: med_gemini.pdf, Page 19, Figure \u001b[0m\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1m6b\u001b[0m                                                                                                           \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mLack of Real-world Validation:\u001b[0m The paper acknowledges the need for \"considerable further research and        \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mdevelopment\" before real-world use. \u001b[1mSource: med_gemini.pdf, Page 19\u001b[0m                                          \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mEthical Considerations:\u001b[0m                                                                                         \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mBias:\u001b[0m  The model's training data could contain biases, leading to inaccurate or unfair diagnoses for certain \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mdemographics.                                                                                                \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mOver-reliance:\u001b[0m Clinicians might over-rely on the model's output, potentially missing crucial nuances in      \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mpatient history or symptoms.                                                                                 \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mPatient Privacy:\u001b[0m  Integrating patient data raises privacy concerns, requiring robust data security and       \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0minformed consent protocols.                                                                                  \n",
              "\n",
              "\u001b[1mSource:\u001b[0m med_gemini.pdf, Pages 18-19, Figure 6                                                                      \n",
              "\n",
              "\u001b[1mQuestion 2:\u001b[0m The paper focuses on uncertainty-guided search for text-based reasoning tasks. How could this approach \n",
              "be extended to multimodal tasks? For instance, if Med-Gemini-M 1.5 encounters uncertainty when analyzing a         \n",
              "dermatology image, could it generate queries to search for relevant visual examples or supplemental clinical       \n",
              "information to refine its interpretation?                                                                          \n",
              "\n",
              "\u001b[1mAnswer:\u001b[0m Yes, uncertainty-guided search can be extended to multimodal tasks.  Med-Gemini-M 1.5 could generate       \n",
              "queries for both visual and textual information to reduce uncertainty.                                             \n",
              "\n",
              "\u001b[1mExplanation:\u001b[0m                                                                                                       \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mCurrent Approach:\u001b[0m  The paper describes using uncertainty-guided search for text-based reasoning. When the model \n",
              "\u001b[1;33m   \u001b[0mis uncertain, it generates search queries to gather more information. \u001b[1mSource: med_gemini.pdf, Page 9\u001b[0m            \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mMultimodal Extension:\u001b[0m This approach can be adapted for multimodal uncertainty:                                  \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mVisual Queries:\u001b[0m  If uncertain about a dermatology image, Med-Gemini-M 1.5 could generate queries like        \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\"prurigo nodularis different skin tones\" or \"dermatofibroma vs. prurigo nodularis images\" to retrieve        \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mrelevant visual examples.                                                                                    \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mSupplemental Information Queries:\u001b[0m It could also generate queries for text-based information, such as \"prurigo\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mnodularis histopathology findings\" or \"differential diagnosis of prurigo nodularis.\"                         \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mRefined Interpretation:\u001b[0m By accessing and integrating both visual and textual search results, the model could    \n",
              "\u001b[1;33m   \u001b[0mimprove its understanding and refine its interpretation of the image.                                           \n",
              "\n",
              "\u001b[1mSource:\u001b[0m med_gemini.pdf, Page 9                                                                                     \n",
              "\n",
              "\u001b[1mQuestion 3:\u001b[0m Considering the potential benefits and risks highlighted in the paper, what specific steps should be   \n",
              "taken during the development, validation, and deployment of Med-Gemini models to ensure they are used safely,      \n",
              "fairly, and effectively in real-world clinical settings? How can these steps be informed by ongoing collaboration  \n",
              "between researchers, clinicians, regulators, and patient communities?                                              \n",
              "\n",
              "\u001b[1mAnswer:\u001b[0m Ensuring safe, fair, and effective use of Med-Gemini models requires a multi-faceted approach throughout   \n",
              "their lifecycle:                                                                                                   \n",
              "\n",
              "\u001b[1mDevelopment:\u001b[0m                                                                                                       \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mDiverse & Representative Data:\u001b[0m  Use training data that reflects diverse patient demographics and clinical       \n",
              "\u001b[1;33m   \u001b[0mpresentations to mitigate bias. \u001b[1mCollaboration:\u001b[0m Engage with patient communities to identify potential data gaps  \n",
              "\u001b[1;33m   \u001b[0mand biases.                                                                                                     \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mTransparency & Explainability:\u001b[0m Develop methods to make the model's reasoning transparent and understandable to  \n",
              "\u001b[1;33m   \u001b[0mclinicians. \u001b[1mCollaboration:\u001b[0m Involve clinicians to determine the level of explainability needed for trust and     \n",
              "\u001b[1;33m   \u001b[0meffective use.                                                                                                  \n",
              "\n",
              "\u001b[1mValidation:\u001b[0m                                                                                                        \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mRigorous Testing:\u001b[0m Conduct extensive testing on diverse datasets and in real-world clinical settings to assess   \n",
              "\u001b[1;33m   \u001b[0maccuracy, safety, and fairness. \u001b[1mCollaboration:\u001b[0m Work with regulators to design appropriate validation studies    \n",
              "\u001b[1;33m   \u001b[0mthat meet regulatory standards.                                                                                 \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mBias Audits:\u001b[0m  Regularly audit the model for biases and implement mitigation strategies. \u001b[1mCollaboration:\u001b[0m          \n",
              "\u001b[1;33m   \u001b[0mCollaborate with ethicists and social scientists to develop comprehensive bias detection methods.               \n",
              "\n",
              "\u001b[1mDeployment:\u001b[0m                                                                                                        \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mHuman Oversight:\u001b[0m Implement systems that ensure human clinicians make final decisions and maintain control over  \n",
              "\u001b[1;33m   \u001b[0mpatient care. \u001b[1mCollaboration:\u001b[0m Work with clinicians to define clear guidelines for human oversight and            \n",
              "\u001b[1;33m   \u001b[0mintervention.                                                                                                   \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mContinuous Monitoring:\u001b[0m Monitor the model's performance in real-world settings and make adjustments as needed.   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1mCollaboration:\u001b[0m Establish feedback mechanisms for clinicians and patients to report issues and contribute to     \n",
              "\u001b[1;33m   \u001b[0mmodel improvement.                                                                                              \n",
              "\n",
              "\u001b[1mOngoing Collaboration:\u001b[0m  Regular communication and collaboration between researchers, clinicians, regulators, and   \n",
              "patient communities are crucial throughout the entire process to ensure responsible and beneficial use of          \n",
              "Med-Gemini models.                                                                                                 \n",
              "\n",
              "\u001b[1mSource:\u001b[0m  med_gemini.pdf (general principles throughout the paper)                                                  \n"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Some questions to try\n",
        "# this time we are not passing any images, but just a simple text query.\n",
        "query = \"\"\"Question 1: Imagine a patient presents with new onset prurigo nodularis.\n",
        "Could Med-Gemini-M 1.5 be used to analyze dermatological images of the patient's lesions in conjunction with a comprehensive history taken\n",
        "from an EHR dialogue to help a clinician reach a diagnosis and develop a treatment plan?\n",
        "What are the limitations and potential ethical considerations of using the model in this way?\n",
        "\n",
        "Question 2: The paper focuses on uncertainty-guided search for text-based reasoning tasks.\n",
        "How could this approach be extended to multimodal tasks?\n",
        "For instance, if Med-Gemini-M 1.5 encounters uncertainty when analyzing a dermatology image, could it generate queries to\n",
        "search for relevant visual examples or supplemental clinical information to refine its interpretation?\n",
        "\n",
        "Question 3:  Considering the potential benefits and risks highlighted in the paper, what specific steps should be taken during the development,\n",
        "validation, and deployment of Med-Gemini models to ensure they are used safely, fairly, and effectively in real-world clinical settings?\n",
        "How can these steps be informed by ongoing collaboration between researchers, clinicians, regulators, and patient communities?\n",
        " \"\"\"\n",
        "\n",
        "(\n",
        "    response,\n",
        "    matching_results_chunks_data,\n",
        "    matching_results_image_fromdescription_data,\n",
        ") = get_answer_from_qa_system(\n",
        "    query,\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    top_n_text=10,\n",
        "    top_n_image=5,\n",
        "    model=multimodal_model_15,\n",
        "    safety_settings=safety_settings,\n",
        "    generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        ")\n",
        "\n",
        "rich_Markdown(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTHdui6jCHzc",
        "outputId": "3ff9d448-5012-490d-c32b-74b3ef21ae59"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "                                                    <span style=\"font-weight: bold; text-decoration: underline\">Question 1:</span>                                                    \n",
              "\n",
              "<span style=\"font-weight: bold\">Question:</span> How does the mixture-of-experts architecture in Gemini 1.5 Pro contribute to its ability to handle long  \n",
              "context while maintaining performance on core capabilities? Discuss the potential trade-offs involved.             \n",
              "\n",
              "<span style=\"font-weight: bold\">Answer:</span> The mixture-of-experts (MoE) architecture enables Gemini 1.5 Pro to handle long contexts by selectively    \n",
              "activating specific expert networks for different parts of the input. This sparsity allows for processing longer   \n",
              "sequences without a proportional increase in computational cost, leading to improved long-context performance while\n",
              "maintaining core capabilities.                                                                                     \n",
              "\n",
              "<span style=\"font-weight: bold\">Explanation:</span>                                                                                                       \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">MoE for Long Context:</span> Traditional transformer models struggle with long sequences due to quadratic complexity.  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>MoE addresses this by using multiple expert networks, each specializing in different aspects of the data. For   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>any given input, only a subset of these experts are activated, enabling efficient processing of longer          \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>sequences. (\"gemini_v1_5_report_technical.pdf\", Page 2)                                                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Maintaining Core Capabilities:</span> The text states that the improved long-context performance \"does not come at the \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>expense of multi-modal core capabilities.\" This suggests that the model's ability to excel in core tasks, likely\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>those requiring shorter context lengths, is maintained. (\"gemini_v1_5_report_technical.pdf\", Page 30)           \n",
              "\n",
              "<span style=\"font-weight: bold\">Potential Trade-offs:</span>                                                                                              \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Complexity:</span> MoE models are inherently more complex to train and require careful routing mechanisms to select    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>appropriate experts, potentially increasing training and inference costs.                                       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Interpretability:</span>  The selective activation of experts can make it harder to interpret the model's              \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>decision-making process compared to standard transformer architectures.                                         \n",
              "\n",
              "\n",
              "                                                    <span style=\"font-weight: bold; text-decoration: underline\">Question 2:</span>                                                    \n",
              "\n",
              "<span style=\"font-weight: bold\">Question:</span> Gemini 1.5 Pro incorporates various safety mitigations, including supervised fine-tuning and             \n",
              "reinforcement learning. Discuss the effectiveness of these mitigations in addressing content safety and            \n",
              "representational harms in both text-to-text and image-to-text modalities. How can these evaluations be improved?   \n",
              "\n",
              "<span style=\"font-weight: bold\">Answer:</span> Not enough context to answer.                                                                              \n",
              "\n",
              "<span style=\"font-weight: bold\">Explanation:</span> While the text mentions the use of safety mitigations, it doesn't provide specific details about their\n",
              "effectiveness in addressing content safety and representational harms.  The provided text focuses on the model's   \n",
              "architecture and performance on various tasks but lacks information on safety evaluations and mitigation           \n",
              "strategies.                                                                                                        \n",
              "\n",
              "\n",
              "                                                    <span style=\"font-weight: bold; text-decoration: underline\">Question 3:</span>                                                    \n",
              "\n",
              "<span style=\"font-weight: bold\">Question:</span> Gemini 1.5 Pro demonstrates surprising in-context language learning capabilities for Kalamang, a         \n",
              "low-resource language. What are the implications of this finding for language preservation and revitalization? What\n",
              "challenges need to be addressed for broader applicability of this approach?                                        \n",
              "\n",
              "<span style=\"font-weight: bold\">Answer:</span> Gemini 1.5 Pro's ability to learn Kalamang translation from limited resources has significant implications \n",
              "for language preservation and revitalization. It suggests large language models can leverage existing linguistic   \n",
              "documentation to bridge the digital divide for under-resourced languages.                                          \n",
              "\n",
              "<span style=\"font-weight: bold\">Explanation:</span>                                                                                                       \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Language Preservation:</span> By learning from grammar books, dictionaries, and parallel texts, Gemini 1.5 Pro can     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>potentially contribute to documenting and preserving endangered languages like Kalamang                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>(\"gemini_v1_5_report_technical.pdf\", Page 13, Image:                                                            \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>\"images/gemini_v1_5_report_technical.pdf_image_4_1_143.jpeg\")                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Revitalization:</span> The model's translation capabilities could facilitate communication and learning materials for  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>speakers of endangered languages, potentially aiding revitalization efforts.                                    \n",
              "\n",
              "<span style=\"font-weight: bold\">Challenges for Broader Applicability:</span>                                                                              \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Resource Availability:</span>  The approach relies on the existence of structured linguistic resources like grammar    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>books and dictionaries, which might not be readily available for all low-resource languages.                    \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Scalability:</span>  Manually creating and curating these resources for every language is time-consuming and           \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>resource-intensive, making it challenging to scale to thousands of languages.                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Quality and Bias:</span>  The quality of the model's output is dependent on the accuracy and comprehensiveness of the  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>provided resources. Biased or incomplete documentation could propagate into the model's translations.           \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "                                                    \u001b[1;4mQuestion 1:\u001b[0m                                                    \n",
              "\n",
              "\u001b[1mQuestion:\u001b[0m How does the mixture-of-experts architecture in Gemini 1.5 Pro contribute to its ability to handle long  \n",
              "context while maintaining performance on core capabilities? Discuss the potential trade-offs involved.             \n",
              "\n",
              "\u001b[1mAnswer:\u001b[0m The mixture-of-experts (MoE) architecture enables Gemini 1.5 Pro to handle long contexts by selectively    \n",
              "activating specific expert networks for different parts of the input. This sparsity allows for processing longer   \n",
              "sequences without a proportional increase in computational cost, leading to improved long-context performance while\n",
              "maintaining core capabilities.                                                                                     \n",
              "\n",
              "\u001b[1mExplanation:\u001b[0m                                                                                                       \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mMoE for Long Context:\u001b[0m Traditional transformer models struggle with long sequences due to quadratic complexity.  \n",
              "\u001b[1;33m   \u001b[0mMoE addresses this by using multiple expert networks, each specializing in different aspects of the data. For   \n",
              "\u001b[1;33m   \u001b[0many given input, only a subset of these experts are activated, enabling efficient processing of longer          \n",
              "\u001b[1;33m   \u001b[0msequences. (\"gemini_v1_5_report_technical.pdf\", Page 2)                                                         \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mMaintaining Core Capabilities:\u001b[0m The text states that the improved long-context performance \"does not come at the \n",
              "\u001b[1;33m   \u001b[0mexpense of multi-modal core capabilities.\" This suggests that the model's ability to excel in core tasks, likely\n",
              "\u001b[1;33m   \u001b[0mthose requiring shorter context lengths, is maintained. (\"gemini_v1_5_report_technical.pdf\", Page 30)           \n",
              "\n",
              "\u001b[1mPotential Trade-offs:\u001b[0m                                                                                              \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mComplexity:\u001b[0m MoE models are inherently more complex to train and require careful routing mechanisms to select    \n",
              "\u001b[1;33m   \u001b[0mappropriate experts, potentially increasing training and inference costs.                                       \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mInterpretability:\u001b[0m  The selective activation of experts can make it harder to interpret the model's              \n",
              "\u001b[1;33m   \u001b[0mdecision-making process compared to standard transformer architectures.                                         \n",
              "\n",
              "\n",
              "                                                    \u001b[1;4mQuestion 2:\u001b[0m                                                    \n",
              "\n",
              "\u001b[1mQuestion:\u001b[0m Gemini 1.5 Pro incorporates various safety mitigations, including supervised fine-tuning and             \n",
              "reinforcement learning. Discuss the effectiveness of these mitigations in addressing content safety and            \n",
              "representational harms in both text-to-text and image-to-text modalities. How can these evaluations be improved?   \n",
              "\n",
              "\u001b[1mAnswer:\u001b[0m Not enough context to answer.                                                                              \n",
              "\n",
              "\u001b[1mExplanation:\u001b[0m While the text mentions the use of safety mitigations, it doesn't provide specific details about their\n",
              "effectiveness in addressing content safety and representational harms.  The provided text focuses on the model's   \n",
              "architecture and performance on various tasks but lacks information on safety evaluations and mitigation           \n",
              "strategies.                                                                                                        \n",
              "\n",
              "\n",
              "                                                    \u001b[1;4mQuestion 3:\u001b[0m                                                    \n",
              "\n",
              "\u001b[1mQuestion:\u001b[0m Gemini 1.5 Pro demonstrates surprising in-context language learning capabilities for Kalamang, a         \n",
              "low-resource language. What are the implications of this finding for language preservation and revitalization? What\n",
              "challenges need to be addressed for broader applicability of this approach?                                        \n",
              "\n",
              "\u001b[1mAnswer:\u001b[0m Gemini 1.5 Pro's ability to learn Kalamang translation from limited resources has significant implications \n",
              "for language preservation and revitalization. It suggests large language models can leverage existing linguistic   \n",
              "documentation to bridge the digital divide for under-resourced languages.                                          \n",
              "\n",
              "\u001b[1mExplanation:\u001b[0m                                                                                                       \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mLanguage Preservation:\u001b[0m By learning from grammar books, dictionaries, and parallel texts, Gemini 1.5 Pro can     \n",
              "\u001b[1;33m   \u001b[0mpotentially contribute to documenting and preserving endangered languages like Kalamang                         \n",
              "\u001b[1;33m   \u001b[0m(\"gemini_v1_5_report_technical.pdf\", Page 13, Image:                                                            \n",
              "\u001b[1;33m   \u001b[0m\"images/gemini_v1_5_report_technical.pdf_image_4_1_143.jpeg\")                                                   \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mRevitalization:\u001b[0m The model's translation capabilities could facilitate communication and learning materials for  \n",
              "\u001b[1;33m   \u001b[0mspeakers of endangered languages, potentially aiding revitalization efforts.                                    \n",
              "\n",
              "\u001b[1mChallenges for Broader Applicability:\u001b[0m                                                                              \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mResource Availability:\u001b[0m  The approach relies on the existence of structured linguistic resources like grammar    \n",
              "\u001b[1;33m   \u001b[0mbooks and dictionaries, which might not be readily available for all low-resource languages.                    \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mScalability:\u001b[0m  Manually creating and curating these resources for every language is time-consuming and           \n",
              "\u001b[1;33m   \u001b[0mresource-intensive, making it challenging to scale to thousands of languages.                                   \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mQuality and Bias:\u001b[0m  The quality of the model's output is dependent on the accuracy and comprehensiveness of the  \n",
              "\u001b[1;33m   \u001b[0mprovided resources. Biased or incomplete documentation could propagate into the model's translations.           \n"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Some questions to try\n",
        "\n",
        "query = \"\"\"Question 1: How does the mixture-of-experts architecture in Gemini 1.5 Pro contribute to its ability to handle long\n",
        "context while maintaining performance on core capabilities? Discuss the potential trade-offs involved.\n",
        "\n",
        "Question 2: Gemini 1.5 Pro incorporates various safety mitigations, including supervised fine-tuning and reinforcement learning.\n",
        "Discuss the effectiveness of these mitigations in addressing content safety and representational harms in both text-to-text and\n",
        "image-to-text modalities. How can these evaluations be improved?\n",
        "\n",
        "Question 3: Gemini 1.5 Pro demonstrates surprising in-context language learning capabilities for Kalamang,\n",
        "a low-resource language. What are the implications of this finding for language preservation and revitalization?\n",
        "What challenges need to be addressed for broader applicability of this approach?\n",
        "\"\"\"\n",
        "(\n",
        "    response,\n",
        "    matching_results_chunks_data,\n",
        "    matching_results_image_fromdescription_data,\n",
        ") = get_answer_from_qa_system(\n",
        "    query,\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    top_n_text=10,\n",
        "    top_n_image=5,\n",
        "    model=multimodal_model_15,\n",
        "    safety_settings=safety_settings,\n",
        "    generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        ")\n",
        "\n",
        "rich_Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwNrHCqbi3xi"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05jynhZnkgxn"
      },
      "source": [
        "Congratulations on making it through this multimodal RAG notebook!\n",
        "\n",
        "While multimodal RAG can be quite powerful, note that it can face some limitations:\n",
        "\n",
        "* **Data dependency:** Needs high-quality paired text and visuals.\n",
        "* **Computationally demanding:** Processing multimodal data is resource-intensive.\n",
        "* **Domain specific:** Models trained on general data may not shine in specialized fields like medicine.\n",
        "* **Black box:** Understanding how these models work can be tricky, hindering trust and adoption.\n",
        "\n",
        "\n",
        "Despite these challenges, multimodal RAG represents a significant step towards search and retrieval systems that can handle diverse, multimodal data."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "building_DIY_multimodal_qa_system_with_mRAG.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}